title: 《统计学习方法》第三章
slug: blog05
date: 2022-07-29
tags: 《统计学习方法》, 机器学习
summary: k 近邻法
status: hidden

## k 近邻模型

k 近邻法将输入实例的特征空间划分为若干子空间，子空间中的若干实例 $x_i$ 同属于 $y_i$ 类别。具体来说，k 近邻法通过在训练集中找到与新输入实例最邻近的 k 个实例，这 k 个实例大部分属于 $y_i$ 类别，就也将新输入实例归属为 $y_i$ 类别。

![k近邻模型](https://storage.live.com/items/4D18B16B8E0B1EDB!7519?authkey=ALYpzW-ZQ_VBXTU)

训练集数据为

$$
T=\{ (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}
$$

其中 $x_i\in\mathbf{R}^n$ 为特征向量，$y_i\in\{c_1,c_2,\cdots,c_K\}$ 为实例类别。k 近邻法根据<em>距离度量</em>，在包括最邻近的 $k$ 个点的邻域 $N_k(x)$ 中确定 $x$ 的 类别 $y$：


$$y=\arg \max_{c_j}\sum_{x_i\in N_k(x)}I(y_i=c_i),\qquad i=1,2,\cdots,N;\ j=1,2,3,\cdots,K$$

其中 $I$ 为指示函数，条件为真时为 $1$，条件为假时为 $0$。

## 策略

### 距离度量

在二维、三维空间中通常使用欧氏距离来衡量两点间的远近关系（相似程度）：

$$d=\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}$$

$$d=\sqrt{(x_2-x_1)^2+(y_2-y_1)^2+(z_2-z_1)^2}$$

在 $\mathbf{R}^n$ 空间中，更通常的距离度量是 $L_p$ 距离，$L_p$ 距离是由距离度量的概念通过推广得到的，因此同样具有衡量两点间远近关系（相似程度）的作用。

设 $x_i,x_j\in\mathbf{R}^n$，$x_i$ 与 $x_j$ 的 $L_p$ 距离由下式给出：

$$L_p(x_i,x_j)=\left(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p\right)^{\frac{1}{p}}$$

### 分类策略

k 近邻法的分类遵循多数表决规则，即输入实例附近 $k$ 个邻近的训练实例的大多数类决定了预测结果。

因此 k 近邻法中的 $k$ 值决定了在一定的距离度量内选取分类『参考点』的数量，可以想知，若选取的 $k$ 值太小，分类结果会对最近邻的几个点敏感，模型就趋于复杂，更容易发生过拟合。

## 算法

### 构造 kd 树

kd 树是一种二叉树，kd 树通过对 $\mathbf{R}^n$ 空间中每一个维度<em>逐次地</em>二分，最终将整个特征空间划分为若干超矩形，kd 树的每一个结点（训练实例）对应于一个超矩形。

**算法 3.2**

> 输入：数据集 $T$
> 输出：kd 树

1. 选择 $x^{(1)}$ 为坐标轴，以所有实例的 $x_i^{(1)}$ 坐标中位数（若中位有两个数则取其中之一）为切分点，将 $x^{(1)}$ 切分为两部分；
2. 重复切分：深度为 $j$ 的结点，选择 $x^{(l)}$ 为切分坐标轴，$l=j\ \mathrm{mod}\ k+1$。对于 $\mathbf{R}^2$ 空间，步骤为 $x^{(1)}\rightarrow x^{(2)}\rightarrow x^{(1)}\rightarrow\cdots$，对于 $\mathbf{R}^3$ 空间，步骤为 $x^{(1)}\rightarrow x^{(2)}\rightarrow x^{(3)}\rightarrow x^{(1)}\rightarrow\cdots$；
3. 直至将所有实例点切分完成。

![构造kd树](https://storage.live.com/items/4D18B16B8E0B1EDB!7521?authkey=ALYpzW-ZQ_VBXTU)

### 搜索 kd 树

**算法 3.3**

> 输入：kd 树，目标点 $x$
> 输出：$x$ 的最近邻

