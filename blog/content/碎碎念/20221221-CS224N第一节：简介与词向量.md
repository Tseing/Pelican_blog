title: CS224n 自然语言处理第一节：简介与词向量
slug: cs224n-lecture1
date: 2022-12-21
tags: NLP, CS224n
summary: CS224n Lecture 1: Introduction and Word Vectors

## one-hot 编码

在传统 NLP 中，词可以使用 one-hot 向量表示，**one-hot** 指向量中只有一个 1，其余都是 0。例如词库为向量

$$\mathrm{vocab}=[\mathrm{hotel\ conference\ motel}]$$

那么单词的 one-hot 向量就是

$$\mathrm{motel}=[0\ 0\ 1]$$

$$\mathrm{hotel}=[1\ 0\ 0]$$

这样得到不同词向量是**正交**的，无法判断词之间的相似性。

## word2vec

- **分布式语义**（distributional semantics）：一个词的语义由最常出现的邻近词决定。

分布式语义是 NLP 中的一个基本概念，基于这个假设，我们可以通过上下文来表示指定的词，得到的词向量又称为**词嵌入**（word embeddings）。

word2vec 是基于分布式语义得到词向量的一种重要方法，其基本设想是

- 拥有一个较大的语料数据库
- 文本中的每一个词都能通过向量表示，语义相似的词具有相似的向量
- 在文本中的位置 $t$ 上，有中心词 $c$ 和上下文词 $o$（为了简化计算，$c$ 对应的 $o$ 通常由窗口 $m$ 决定）
- 使用 $c$ 和 $o$ 的词向量相似性计算给定 $c$ 时 $o$ 的概率
- 不断调整词向量直至<dot>最大化</dot>该概率（最大似然法）

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8285?authkey=ALYpzW-ZQ_VBXTU)

在文本中的位置 $t=1,\cdots,T$ 上，对中心词 $w_j$ 固定窗口大小为 $m$，那么文本数据的似然为

$$L(\theta)=\prod^T_{t=1}\prod_{\substack{-m\leq j\leq m\\j\neq0}}P(w_{t+j}|w_t;\theta)$$

最大似然的目标就是 $\max L(\theta)$，在这里取负对数将其转化为

$$J(\theta)=-\frac{1}{T}\log L(\theta)=-\frac{1}{T}\prod^T_{t=1}\prod_{\substack{-m\leq j\leq m\\j\neq0}}P(w_{t+j}|w_t;\theta)$$

优化目标就变为 $\min J(\theta)$，更加符合最优化的习惯，这里的 $J(\theta)$ 就是 word2vec 的损失函数。

$J(\theta)$ 中的条件概率一项如何计算呢？这里为了便于计算，当 $w$ 为中心词时用 $v_w$ 表示，当 $w$ 为上下文词时用 $u_w$ 表示。

{note begin}每一个词 $w$ 都会具有两个不同的向量 $v_w$ 和 $u_w$，在计算结束后，这两个向量会非常相似，通常取这两个向量的平均得到词向量。{note end}

那么对于任意指定的中心词 $c$ 和相应的上下文词 $o$，就有条件概率

$$P(o|c)=\frac{\exp(u_o^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}$$

该条件概率同时也表示当两个词的向量点积越大，两个词临近的概率 $P(o|c)$ 越大，这正是 word2vec 的基本思想。该式可以与 softmax 函数类比：

$$\mathrm{softmax}(x_i)=\frac{\exp(x_i)}{\sum_{j=1}^n\exp(x_j)}=p_i$$

其本质就是将中心词 $c$ 和上下文词 $o$ 的相似性 $u_o^Tv_c$ 转化为<dot>概率</dot>形式（非负且和为 1）。

在得到 $P(o|c)$ 后，就可以采用梯度下降的方法实现 $\min J(\theta)$，所以需要计算每个目标词 $v_c$ 的 $\frac{\partial}{\partial v_c}\log P(o|c)$，推导过程如下：

$$\begin{align}
    \frac{\partial}{\partial v_c}\log P(o|c)&=\frac{\partial}{\partial v_c}\log\frac{\exp(u_o^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}\\
    &=\frac{\partial}{\partial v_c}\left[\log \exp(u_o^Tv_c)-\log\sum_{w\in V}\exp(u_w^Tv_c)\right]\\
    &=\frac{\partial}{\partial v_c}u_o^Tv_c-\frac{\partial}{\partial v_c}\log\sum_{w\in V}\exp(u_w^Tv_c)
\end{align}$$

注意 $v_c$ 为向量，由向量求导法则 $\frac{\mathrm{d}Ax}{\mathrm{d}x}=A^T$，可以得到

$$
\begin{align}
    \frac{\partial}{\partial v_c}\log P(o|c)&=u_o-\frac{\partial}{\partial v_c}\log\sum_{w\in V}\exp(u_w^Tv_c)\\
    &=u_o-\frac{1}{\sum_{w\in V}\exp(u_w^Tv_c)}\left[\frac{\partial}{\partial v_c}\sum_{x\in V}\exp(u_x^Tv_c)\right]\\
    &=u_o-\frac{1}{\sum_{w\in V}\exp(u_w^Tv_c)}\sum_{x\in V}\frac{\partial}{\partial v_c}\exp(u_x^Tv_c)\\
    &=u_o-\frac{1}{\sum_{w\in V}\exp(u_w^Tv_c)}\sum_{x\in V}\exp(u_x^Tv_c)u_x\\
    &=u_o-\sum_{x\in V}\frac{\exp(u_x^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}u_x\\
    &=u_o-\sum_{x\in V}P(x|c)u_x
\end{align}
$$

实际上 $\sum_{x\in V}P(x|c)u_x$ 就是 $u_x$ 的期望，使用 softmax 的好处就在于能将最优化问题转化为最小化形如「观察值 – 预测值」的误差。

---

## References

- [CS224n: Natural Language Processing with Deep Learning](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/)