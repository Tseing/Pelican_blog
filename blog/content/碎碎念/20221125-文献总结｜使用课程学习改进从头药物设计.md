title: 文献总结｜使用课程学习改进从头药物设计
slug:  summary-doi.org/10.1038/s42256-022-00494-4
date: 2022-11-25
tags: Literature Summary, CADD, RNN
summary: 本文介绍于 2022 年发表在 Nature Machine Intelligence 上的一篇文章，文章原标题为 Improving De Novo Molecular Design with Curriculum Learning，文章介绍了深度学习中的课程学习，并将课程学习的策略用于 REINVENT 模型，相比于强化学习的策略，课程学习对解决复杂任务具有更好效果。

<i class="fa fa-external-link"></i> [doi.org/10.1038/s42256-022-00494-4](https://doi.org/10.1038/s42256-022-00494-4)

本文介绍于 2022 年发表在 *Nature Machine Intelligence* 上的一篇文章，文章原标题为 Improving De Novo Molecular Design with Curriculum Learning，文章介绍了深度学习中的课程学习，并将课程学习的策略用于 REINVENT 模型，相比于强化学习的策略，课程学习对解决复杂任务具有更好效果。

## 引言

强化学习是深度学习中的常用策略，而强化学习在面对较为复杂的任务时，奖励函数的计算将消耗大量资源，不仅效率低下，还有很大可能无法完成相应任务。课程学习是强化学习的一种替代方法，它将复杂任务分为连续的若干任务，通过这种方式节省了计算资源，同时模型也更容易学习简单的任务。

包括 REINVENT 在内的许多药物分子生成模型都使用了强化学习策略，其中的奖励函数通常包括例如分子对接打分和理化性质预测等复杂的因素，使用这样复杂的奖励函数，模型很难通过学习过程找到最优解，最终偏离预期目标。

因此文章在 REINVENT 模型中引入了课程学习策略，设计了以生成 PDK1 抑制剂为目标的复杂任务，对比与强化学习策略效果的差异。文章结果表明，课程学习绕过了包含分子对接在内的复杂奖励函数，大大提高了计算效率。

## 方法

### 课程学习原理

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8089?authkey=ALYpzW-ZQ_VBXTU)

课程学习的过程可以分为课程阶段与输出阶段，在课程阶段中，模型需要学习逐渐复杂的连续的任务，在输出阶段中，模型输出满足满足最终要求的分子。

**打分函数** $S:\rm{SMILES}\rightarrow[0,1]$ 由加权几何平均构成：

$$S(x)=\left[\prod^n_{i=1}c_i(x)^{w_i}\right]^{1/\sum^n_{i=1}w_i}$$

其中 $c_i:\rm{SMILES}\rightarrow[0,1]$，$w_i$ 是 $c_i$ 的相应权值。$S(x)$ 是生成化合物 $x$ 的打分函数，基梯度用于更新代理模型。

**课程** $C$ 由目标序列构成，$O=\{O_{C_1},\cdots,O_{C_{n-1}},O_{C_n},O_P\}$，其中 $C$ 表示课程目标，$P$ 表示生成目标。对于每个目标，都有相应的 $S(x)$ 用于更新代理模型。

课程阶段由若干个课程单元构成，每个单元中的强化学习目标是输出满足该单元课程学习目标 $O_{C_i}$ 的分子，只有打分高于阈值的分子才会进入下一个课程单元，经过连续的课程单元，生成分子的复杂性也增加（任务复杂性也增加）。完成全部连续的课程学习目标 $O_{C_n}$ 后，进入生成阶段，此时强化学习的目标是生成任务的目标 $O_P$，最后输出符合预期的分子。

### 模型

文章中使用的模型是 REINVENT 生成模型，主要由 RNN 构成，REINVENT 模型的先验模型已经使用 ChEMBL 数据集进行预训练，能够生成合法的 SMILES 分子。

模型中同样使用了 REINVENT 原有的多样性过滤器，在课程阶段，为了保证能够准确获得先验知识，不使用过滤器，为了使输出分子具有尽可能的多样性，在输出阶段使用过滤器。

关于 REINVENT 的更多细节，参考[前文](https://tseing.github.io/sui-sui-nian/2022-10-15-summary-doi.org/10.1021/acs.jcim.0c00915.html)。

## 结果

文章设计了三种实验对比强化学习与课程学习的效果差异：

**实验 1**：生成具有目标骨架的分子。

**实验 2**：生成满足分子对接约束的分子，使用谷本相似性（2D）作为打分函数

**实验 3**：生成满足分子对接约束的分子，使用 ROCS 3D 相似性作务打分函数。

对于实验 2 和实验 3，将进入生成阶段的阈值设为低（0.5）和高（2D: 0.8, 3D: 0.75）两种模式，对比结果差异。

#### 实验 1

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8090?authkey=ALYpzW-ZQ_VBXTU)

实验 1 的目标是生成包含二氢吡唑喹唑啉结构的分子。在使用 RL 策略的模型中，在 2000 个 epoch 的过程中，分子打分基本上都是 0.5（不包含该子结构），表明模型没能生成具有该结构的分子。而在使用 CL 策略的模型中，尽管目标十分复杂，在经过 5 个课程目标后，分子的阈值超过了 0.8，生成了具有该结构的分子。

CL 过程中，分子打分不断上升，当分数超过 0.8 的阈值，即完成当前课程目标后，分子打分迅速下降，进入下一个课程目标。还可以从化合物的结构中看出，随着 CL 过程的进行，分子结构变得更加复杂，最终满足任务目标。上述结果明确展示了 CL 逐个完成连续目标的过程。

此外，通过 CL 过程生成目标分子只经过了 1500 个 epoch，计算效率远远高于 RL 过程。

#### 实验 2 和 3

实验 2 和 3 都加入了分子对接的约束，目标是生成与 PDK1 受体 Ala 162 具有两个氢键作用且相比参考配体具有更强的预测活性（对接打分）和类药性（QED） 的分子，两个实验的唯一区别在于分别使用了 2D 和 3D 的打分函数。

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8091?authkey=ALYpzW-ZQ_VBXTU)

在 0-100 epoch 中，RL 过程的分数一直很低，直到 100-200 epoch，RL 过程生成分子的分数才开始上升，但分数还是显著低于 CL 方法生成的分子。

可以看出，使用谷本相似性或 ROCS 相似性作为模型的打分函数，都能指引模型完成复杂的生成目标。从完成生成目标的过程看来，两种打分函数没有什么区别，表现出相似的性能。

而使用低分模式与高分模式谷本相似性的模型有所区别，使用低分模式的模型在刚开始阶段的分数很低，但是在 50 个 epoch 内迅速上升，赶上高分模型的打分。使用低分模式与高分模式 ROCS 相似性的模型也有相似的表现，但 ROCS 相似性模型开始阶段的分数没有谷本相似性模型高，这是由于从 SMILES 数据中学习 3D 构象有一定困难。

在生成满足分子对接约束分子的任务上，CL 策略也顺利完成了目标。

#### 优化效果

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8092?authkey=ALYpzW-ZQ_VBXTU)

对多种策略生成的结果分子进行分子对接，评估分子活性，可以看出，与相比 RL，CL 过程能够生成更大量的预期分子，在诸多 CL 过程中，使用高分模式的模型生成的分子活性更高（约 -11 kcal/mol）。

#### 骨架多样性

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8093?authkey=ALYpzW-ZQ_VBXTU)

使用 Bemis-Murcko 分子骨架评估不同方式生成分子的多样性，在结果中也可以明显看出，CL 过程生成了更多独特的分子骨架，以谷本相似性作为打分函数的模型生成分子的多样性更高，高分模式生成分子的多样性更高。

## 结论

文章基于 REINVENT 模型，引入了课程学习策略，大大加快了模型解决多参数优化的问题的效率。强化学习策略在面对复杂任务时通常效率低下，甚至无法解决，实验结果证明引入 CL 策略的模型顺利解决了生成目标，完成了构造具有特定结构分子与生成满足对接要求分子的两种任务。课程学习策略有助于分子生成模型完成复杂任务并提高计算效率。