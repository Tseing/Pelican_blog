title: ä»é›¶èµ·æ­¥çš„ Transformer ä¸ä»£ç æ‹†è§£
slug: transformer-from-scratch
date: 2023-04-21
tags: Python, PyTorch, Transformer

è‡ª Google çš„è®ºæ–‡ [Attention Is All You Need](https://arxiv.org/abs/1706.03762) å‘å¸ƒåï¼Œå‡ å¹´å†…æ¶Œç°äº†å¤§é‡åŸºäº Transformer çš„æ¨¡å‹ï¼Œä¿¨ç„¶å½¢æˆäº† Transformer æ¨ªæ‰«äººå·¥æ™ºèƒ½é¢†åŸŸçš„æ€åŠ¿ã€‚

ç½‘ç»œä¸Šä¹Ÿå‡ºç°äº†å¤§é‡è§£è¯»è®ºæ–‡æˆ–æ˜¯è®²è§£ Transformer çš„æ–‡ç« ï¼Œå…¶ä¸­ä¹Ÿä¸ä¹è®¸å¤šé«˜æ°´å¹³äººå·¥æ™ºèƒ½ä»ä¸šè€…çš„è§£è¯»ã€‚è™½ç„¶æœ‰äº›å¯ä»¥ç§°å¾—ä¸Šæ˜¯é«˜å±‹å»ºç“´ï¼Œä½†ç›¸å½“å¤§éƒ¨åˆ†éš¾ä»¥é¿å…åœ°è½å…¥äº†çŸ¥è¯†çš„è¯…å’’ï¼ˆcurse of knowledgeï¼‰ï¼Œèµ·ç åœ¨æˆ‘åˆå¼€å§‹äº†è§£ Transformer æ—¶éš¾ä»¥è¯»æ‡‚è¿™äº›æ–‡ç« ã€‚

éšç€ Transformer å¹¿æ³›åº”ç”¨åˆ°å„é¢†åŸŸï¼Œå­¦ä¹  Transformer ä¹Ÿæˆäº†ä¸€é—¨ã€Œæ˜¾å­¦ã€ã€‚å°½ç®¡æˆ‘å·²ç»èƒ½è¯»æ‡‚ä¸€äº›æ›´æ·±å±‚æ¬¡çš„ Transformer å‰–æï¼Œä½†æˆ‘è¿˜æ˜¯æœªæ‰¾è§ä¸€ç¯‡åˆæˆ‘å¿ƒæ„çš„å…¥é—¨æ–‡ç« ï¼Œæ‰€ä»¥æˆ‘å¸Œæœ›èƒ½æ’°å†™ä¸€ç¯‡å°æ–‡ç« ï¼Œä»¥åˆå­¦è€…çš„è§’åº¦æ¥è®²è§£ Transformerï¼Œæ˜¯ä¸ºåºã€‚

## æ¥”å­

Transformer æ˜¯è®¾è®¡ç”¨äº NLP çš„ä¸€ç§æ¨¡å‹ï¼Œå°½ç®¡ç›®å‰ Transformer æ‰€èƒ½å®Œæˆçš„ä»»åŠ¡å·²ç»å¤§å¤§æ‰©å±•ï¼Œä½†è¿™é‡Œè¿˜æ˜¯ä»¥æœ€åŸå§‹çš„ç¿»è¯‘ä»»åŠ¡ä¸ºä¾‹ã€‚

åœ¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œæ‰€éœ€è¦çš„æ•°æ®åŒ…æ‹¬åŸå§‹è¯­å¥ä¸ç›®æ ‡è¯­å¥ï¼Œä¹Ÿå°±æ˜¯ Transformer åŸè®ºæ–‡ä¸­æ‰€æŒ‡çš„ã€Œinputã€å’Œã€Œoutputã€ï¼Œå› ä¸ºåå­—å¤ªå®¹æ˜“æ··æ·†ï¼Œè¿˜æ˜¯å°†å…¶åŸå§‹è¯­å¥ä¸ç›®æ ‡è¯­å¥æˆ–æ˜¯ã€Œsourceã€ä¸ã€Œtargetã€ã€‚

å‡è®¾ source ä¸º `ä½ å¥½ï¼Œä¸–ç•Œï¼`ï¼Œtarget ä¸º `Hello, world!`ï¼Œå®Œæˆè¿™ä¸ªä¸­è¯‘è‹±ä»»åŠ¡é¦–å…ˆè¦å°†æ–‡æœ¬è½¬åŒ–ä¸ºåˆ©äºæ¨¡å‹å¤„ç†çš„æ•°å€¼ï¼Œè¿™ä¸€æ­¥ç§°ä¸ºè¯åµŒå…¥ï¼ˆembeddingï¼‰ã€‚

å¸¸è§çš„è¯åµŒå…¥æ–¹æ³•æœ‰ word2vec ç­‰ç­‰ï¼Œåœ¨è¿™é‡Œä¸åšä»‹ç»ã€‚è¯åµŒå…¥æ­¥éª¤å¤§è‡´çš„æµç¨‹æ˜¯å…ˆå°† `ä½ å¥½ï¼Œä¸–ç•Œï¼` è½¬åŒ–ä¸º `<start> ä½ å¥½ ï¼Œ ä¸–ç•Œ ï¼ <end>`ï¼Œæ¯ä¸ªã€Œè¯ã€éƒ½ç”¨ç©ºæ ¼åˆ’åˆ†å¼€ï¼Œå…¶ä¸­ `<start>` ä¸ `<end>` åˆ†åˆ«è¡¨ç¤ºæ–‡æœ¬çš„èµ·è®«ï¼Œè¿™äº›ã€Œè¯ã€åœ¨ NLP é€šå¸¸ç§°ä¸ºã€Œtokenã€ã€‚æ¥ç€å†ä¸ºæ¯ä¸ª token åˆ†é…ç´¢å¼•ï¼Œä¾‹å¦‚ `<start>` ä¸º `1`ï¼Œ`<end>`ä¸º `0`ï¼Œç…§è¿™ä¸ªæ€è·¯ï¼Œæ–‡æœ¬å°±å¯ä»¥è½¬æ¢ä¸º `[1 2 3 4 5 0]` çš„è¡¨ç¤ºã€‚å½“ç„¶è¿™æ˜¯å¾ˆç®€å•çš„åšæ³•ï¼Œå®é™…ä¸Šï¼Œæ¯ä¸ª token éƒ½ä¼šè¢«è½¬åŒ–ä¸ºæŒ‡å®šç»´åº¦çš„å‘é‡ï¼Œç”¨è¿™ä¸€è¿ä¸²å‘é‡å°±å¯ä»¥è¡¨ç¤ºæ–‡æœ¬ã€‚

å°†ä¸Šè¿°è¿‡ç¨‹æŠ½è±¡å‡ºæ¥ï¼Œåœ¨è¯åµŒå…¥åï¼Œå¯ä»¥å¾—åˆ° source çš„è¡¨ç¤º $\boldsymbol{X}=(\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_t)$ ä¸ target çš„è¡¨ç¤º $\boldsymbol{Y}=(\boldsymbol{y}_1,\boldsymbol{y}_2,\cdots,\boldsymbol{y}_t)$ï¼Œå…¶ä¸­ $\boldsymbol{x}_i$ ä¸ $\boldsymbol{y}_i$ éƒ½æ˜¯æŒ‡å®šç»´åº¦ $d$ çš„å‘é‡ã€‚

é‚£ä¹ˆå¦‚ä½•ä½¿ç”¨ $\boldsymbol{X}$ ä¸ $\boldsymbol{Y}$ å®Œæˆç¿»è¯‘ä»»åŠ¡å‘¢ï¼Ÿ

**ç¬¬ä¸€ç§**æ˜¯ä½¿ç”¨ RNN æ–¹æ³•ï¼Œä½¿ç”¨å½“å‰çš„ source token $\boldsymbol{x}_t$ ä¸å‰ä¸€æ­¥ä¸­ç”Ÿæˆçš„ token $\hat{\boldsymbol{y}}_{t-1}$ ç”Ÿæˆä¸‹ä¸€ä¸ª tokenï¼Œé€ä¸ªç”Ÿæˆç›´è‡³å¥å­æœ«å°¾ï¼š

$$\hat{\boldsymbol{y}}_t=f(\hat{\boldsymbol{y}}_{t-1},\boldsymbol{x}_t)$$

**ç¬¬äºŒç§**æ˜¯ä½¿ç”¨å·ç§¯çš„æ–¹æ³•ï¼Œå®šä¹‰ä¸€ä¸ªçª—å£é•¿åº¦å†é€šè¿‡å°èŒƒå›´ä¸­çš„å‡ ä¸ª $\boldsymbol{x}_i$ è®¡ç®—è¾“å‡ºï¼š

$$\hat{\boldsymbol{y}}_t=f(\boldsymbol{x}_{t-1},\boldsymbol{x}_t,\boldsymbol{x}_{t+1})$$

å¯ä»¥çœ‹å‡ºï¼Œ<dot>RNN å¾ˆéš¾å­¦ä¹ åˆ°å…¨å±€çš„ä¿¡æ¯</dot>ï¼Œè€Œ<dot>å·ç§¯æ–¹æ³•åªèƒ½å­¦ä¹ åˆ°å°èŒƒå›´çš„å±€éƒ¨ä¿¡æ¯</dot>ã€‚

æ‰€ä»¥ Transformer ç»™å‡ºäº†**ç¬¬ä¸‰ç§**æ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯è‡ªæ³¨æ„åŠ›æ–¹æ³•ã€‚è‡ªæ³¨æ„åŠ›æœºåˆ¶è®©æ¨¡å‹å°±å½“å‰çš„ source token $\boldsymbol{x}_t$ ä¸ $\boldsymbol{X}$ ä¸­å…¶ä»– token çš„å…³ç³»ç»™å‡ºè¾“å‡º $\hat{\boldsymbol{y}}_t$ï¼š

$$\hat{\boldsymbol{y}}_t=f(\boldsymbol{x}_t, \boldsymbol{X})$$

## Transformer ç»“æ„

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU)

æ ‡å‡† Transformer çš„ç»“æ„å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå¤§è‡´åˆ†ä¸ºå·¦ä¾§çš„ Encoder ä¸å³ä¾§çš„ Decoder ä¸¤ä¸ªéƒ¨åˆ†ã€‚Inputs ä¸ Outputs åˆ†åˆ«æ˜¯ä¸Šæ–‡æ‰€è¯´çš„ source ä¸ targetï¼ŒOutput Probabilities æ˜¯æ¨¡å‹è¾“å‡ºçš„å„ token æ¦‚ç‡ï¼Œå–å…¶ä¸­æœ€å¤§æ¦‚ç‡çš„ token å°±èƒ½ç»„ç»‡æˆæ¨¡å‹è¾“å‡ºç»“æœã€‚

### ä½ç½®ç¼–ç 

Transformer å¹¶æ²¡æœ‰é‡‡ç”¨ RNN ä¸å·ç§¯æ–¹æ³•æ‰€ä½¿ç”¨çš„åºåˆ—å¤„ç† token çš„æ–¹æ³•ï¼Œå› è€Œèƒ½å¤Ÿå®ç°å¹¶è¡Œè®¡ç®—å¹¶ä¸”å¾ˆå¤§ç¨‹åº¦ä¸Šç¼“è§£äº†é•¿æœŸä¾èµ–é—®é¢˜ï¼ˆé¡ºåºå¤„ç†é•¿åºåˆ—å®¹æ˜“ä¸¢å¤±å¤šä¸ªæ­¥éª¤å‰çš„ä¿¡æ¯ï¼‰ã€‚æ–‡æœ¬ä¸­å¤šä¸ª token é—´æ˜¾ç„¶æœ‰å‰åçš„é¡ºåºå…³ç³»ï¼ŒTransformer ä½¿ç”¨ä½ç½®ç¼–ç çš„æ–¹å¼æ¥å¤„ç†é¡ºåºä¿¡æ¯ã€‚

source ä¸ target é€å…¥æ¨¡å‹ï¼Œç»è¿‡å¸¸è§„çš„è¯åµŒå…¥è¿‡ç¨‹åï¼Œè¿˜éœ€è¦åœ¨å¾—åˆ°çš„çŸ©é˜µä¸ŠåŠ ä¸Šä½ç½®ç¼–ç ï¼Œè®ºæ–‡å°†ä½ç½®ç¼–ç å®šä¹‰ä¸º

$$\mathrm{PE}_{(\mathrm{pos},2i)}=\sin(\mathrm{pos}/10000^{2i/d_\mathrm{model}})$$

$$\mathrm{PE}_{(\mathrm{pos},2i+1)}=\cos(\mathrm{pos}/10000^{2i/d_\mathrm{model}})$$

Transformer å°† $\mathrm{pos}$ ä½ç½®æ˜ å°„ä¸º $d_\mathrm{model}$ ç»´çš„å‘é‡ï¼Œå‘é‡ä¸­çš„ç¬¬ $i$ ä¸ªå…ƒç´ å³æŒ‰ä¸Šå¼è®¡ç®—ã€‚ä½ç½®ç¼–ç çš„è®¡ç®—å…¬å¼æ˜¯æ„é€ å‡ºçš„ç»éªŒå…¬å¼ï¼Œä¸å¿…æ·±ç©¶ï¼Œå½“ç„¶ä¹Ÿæœ‰è®¸å¤šæ–‡ç« åˆ†æäº†å¦‚æ­¤æ„é€ çš„åŸå› ï¼Œè¿™é‡Œä»ç•¥ã€‚

### Encoder ä¸ Decoder

è®¸å¤šå®Œæˆ seq2seq ä»»åŠ¡çš„æ¨¡å‹éƒ½é‡‡ç”¨äº† encoder-decoder æ¨¡å¼ï¼ŒTransformer ä¹Ÿä¸ä¾‹å¤–ã€‚ç®€å•æ¥è¯´ï¼Œencoder å°†è¾“å…¥ç¼–ç å¾—åˆ°ä¸€ä¸ªä¸­é—´å˜é‡ï¼Œdecoder è§£ç è¯¥ä¸­é—´å˜é‡å¾—åˆ°è¾“å‡ºã€‚

åœ¨ Transformer ä¸­ï¼Œsource ä¸ target åˆ†åˆ«é€å…¥ encoder ä¸ decoderï¼Œencoder è®¡ç®—å¾—åˆ°çš„ä¸­é—´ç»“æœå†é€å…¥ decoder ä¸­ä¸ target è¾“å…¥è¿›è¡Œè®¡ç®—ï¼Œå¾—åˆ°æœ€åçš„ç»“æœï¼Œè¿™å°±æ˜¯æ‰€è°“ã€Œç¼–ç -è§£ç ã€çš„å·¥ä½œæ–¹å¼ã€‚

ä» Transformer çš„ç»“æ„å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œæ¨¡å‹å…·æœ‰ $N$ å±‚ encoder ä¸ decoder å±‚ã€‚å…¶ä¸­ï¼Œencoder ä¸ decoder éƒ½å…·æœ‰ç›¸åŒçš„å¤šå¤´æ³¨æ„åŠ›å±‚ï¼ˆMulti-Head Attentionï¼‰ã€å‰é¦ˆå±‚ï¼ˆFeed Forwardï¼‰ã€‚encoder ä¸ decoder çš„ä¸åŒåœ¨äº decoder å¤šäº†ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚ï¼Œåœ¨è¿™ä¸€å±‚ä¸­ï¼Œencoder çš„è¾“å‡ºä¸ decoder çš„è¾“å…¥è®¡ç®—æ³¨æ„åŠ›ã€‚

è¿˜å¯ä»¥æ³¨æ„åˆ°ï¼Œåœ¨ encoder ä¸ decoder ä¸­ï¼Œæ¯ä¸€å±‚åéƒ½æœ‰ä¸€ä¸ª Add & Norm å±‚ï¼Œç”¨äºå½’ä¸€åŒ–è®¡ç®—ç»“æœã€‚Add & Norm å±‚çš„è®¡ç®—æ–¹å¼æ˜¯å°†å‰ä¸€å±‚çš„è¾“å…¥ä¸å‰ä¸€å±‚çš„è¾“å‡ºç›¸åŠ ï¼Œç„¶åå½’ä¸€åŒ–ï¼Œå¯ä»¥è¡¨ç¤ºä¸º $\mathrm{LayerNorm}(\boldsymbol{x}+\mathrm{Sublayer}(\boldsymbol{x}))$ã€‚

#### Attention æœºåˆ¶

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU)

æ•°æ®è¿›å…¥ encoder ä¸ decoder çš„å†…éƒ¨ï¼Œé¦–å…ˆè¦é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œè®¡ç®—ï¼Œè¿™ä¹Ÿæ˜¯ Transformer çš„æ ¸å¿ƒã€‚

æ–‡ç« ä¸­å°†æ‰€ä½¿ç”¨çš„æ³¨æ„åŠ›ç§°ä¸ºç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆscaled dot-product attentionï¼‰ï¼Œå®šä¹‰ä¸º

$$\mathrm{Attention}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = \mathrm{Softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^\top}{\sqrt{d_k}}\right)\boldsymbol{V}$$

å…¶ä¸­ $\boldsymbol{Q}_{n\times d_k}$ã€$\boldsymbol{K}_{m\times d_k}$ã€$\boldsymbol{V}_{m\times d_v}$ åˆ†åˆ«æ˜¯è‹¥å¹²å‘é‡ $\boldsymbol{q}\in\mathbb{R}^{d_k}$ã€$\boldsymbol{k}\in\mathbb{R}^{d_k}$ã€$\boldsymbol{v}\in\mathbb{R}^{d_v}$ ç»„æˆçš„çŸ©é˜µã€‚

å•çœ‹çŸ©é˜µçš„ä¹˜æ³•ç¨æ˜¾å¤æ‚ï¼Œä¸å¦¨å…ˆç”¨å‘é‡è¯´æ˜è®¡ç®—æ­¥éª¤ã€‚é€šè¿‡ä»¥ä¸‹æ–¹å¼å¯ä»¥ä»è¾“å…¥ $\boldsymbol{x}$ å¾—åˆ°å‘é‡ $\boldsymbol{q}$ã€$\boldsymbol{k}$ã€$\boldsymbol{v}$ï¼š

$$\boldsymbol{q}=\boldsymbol{x}\boldsymbol{W}^Q,\,\boldsymbol{k}=\boldsymbol{x}\boldsymbol{W}^K,\,\boldsymbol{v}=\boldsymbol{x}\boldsymbol{W}^V$$

å…¶ä¸­ï¼Œ$\boldsymbol{W}^Q$ã€$\boldsymbol{W}^K$ã€$\boldsymbol{W}^V$ åˆ†åˆ«è¡¨ç¤ºç›¸åº”çš„æƒé‡çŸ©é˜µã€‚$\boldsymbol{q}$ ä»£è¡¨ queryï¼Œ$\boldsymbol{k}$ ä»£è¡¨ keyï¼Œ$\boldsymbol{v}$ ä»£è¡¨ valueï¼Œç›®çš„æ˜¯<dot>ç”¨ query å»å¯»æ‰¾æ›´åŒ¹é…çš„ key-value å¯¹</dot>ã€‚

å› ä¸ºæ•°é‡ç§¯å¯ä»¥è¡¨ç¤ºä¸¤å‘é‡çš„ç›¸ä¼¼ç¨‹åº¦ï¼Œä¸€ç§ç®€å•çš„åšæ³•æ˜¯ä½¿ç”¨ $\boldsymbol{q}$ ä¸è‹¥å¹²ä¸ª $\boldsymbol{k}$ è®¡ç®—æ•°é‡ç§¯ï¼Œå°†å…¶ä½œä¸ºåŒ¹é…åˆ†æ•°ï¼š

$$\mathrm{score}=\boldsymbol{q}\cdot \boldsymbol{k}_i=\boldsymbol{q}\boldsymbol{k}^\top_i$$

ä½†è¿™æ ·çš„ã€Œæ³¨æ„åŠ›ã€å¤ªè¿‡äºç®€å•ï¼ŒGoogle ä»ä¸Šè¿°çš„æ•°é‡ç§¯å‡ºå‘ï¼Œè®¾è®¡äº†æ›´ä¸ºå¯é çš„æ³¨æ„åŠ›ï¼š

$$\mathrm{Attention}(\boldsymbol{q},\boldsymbol{k}_i,\boldsymbol{v}_i)=\frac 1 Z\sum_i\exp\left(\frac{\boldsymbol{q}\boldsymbol{k}^\top_i}{\sqrt{d_k}}\right)\boldsymbol{v}_i$$

é¦–å…ˆï¼Œå¼ä¸­ $1/Z\sum_i x_i$ å½¢å¼çš„éƒ¨åˆ†æ˜¯ Softmax å‡½æ•°çš„ç®€å†™ï¼ŒSoftmax å‡½æ•°ç”±ä¸‹å¼å®šä¹‰ï¼š

$$\mathrm{Softmax}(x_i)=\frac{\exp(x_i)}{\sum_j\exp(x_j)}$$

Softmax å‡½æ•°çš„ä½œç”¨æ˜¯å°†è‹¥å¹²æ•°å€¼ $x_i$ å½’ä¸€åŒ–ï¼Œå¾—åˆ°çš„ $\mathrm{Softmax}(x_i)$ å…·æœ‰

- $\sum_i\mathrm{Softmax}(x_i)=1$
- $\mathrm{Softmax}(x_i)\in[0, 1]$

ä¸¤ç‚¹æ€§è´¨ï¼Œæ‰€ä»¥ä¸æ¦‚ç‡å…·æœ‰ç›¸ä¼¼çš„ç‰¹å¾ï¼Œå¯ä»¥ç”¨ä½œæ¦‚ç‡å¤„ç†ã€‚

å…¶æ¬¡ï¼Œå¼ä¸­æ–°å¢çš„ $\sqrt{d_k}$ ç”¨äºè°ƒèŠ‚å†…ç§¯ $\boldsymbol{q}\boldsymbol{k}^\top_i$ çš„å¤§å°ã€‚å½“è‹¥å¹²å†…ç§¯çš„å¤§å°è¿‡äºæ‚¬æ®Šæ—¶ï¼ŒSoftmax å‡½æ•°å¾ˆå®¹æ˜“å°†å…¶æ¨å‘ $0$ æˆ– $1$ çš„è¾¹ç•Œå€¼ï¼Œè¿™æ ·çš„æ•°å€¼å¤„ç†èµ·æ¥æ²¡ä»€ä¹ˆæ„ä¹‰ã€‚

æœ€åï¼Œå†æ¬¡å›å¿† Transformer çš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯ç”¨ query å»å¯»æ‰¾æ›´åŒ¹é…çš„ key-value å¯¹ã€‚é‚£ä¹ˆä¸Šå¼çš„æ„ä¹‰å°±å¾ˆäº†ç„¶äº†ï¼Œå°±æ˜¯å°† query ä¸å„ä¸ª key çš„åŒ¹é…åˆ†æ•°è½¬åŒ–ä¸ºå„ä¸ªæ¦‚ç‡ï¼Œå†æŒ‰å„ä¸ªæ¦‚ç‡å–å„ä¸ª key æ‰€å¯¹åº”çš„ valueï¼Œç»„åˆå„ value åˆ†é‡å³å¾—åˆ°æ³¨æ„åŠ›ã€‚

ä»¥å…·æœ‰ä¸¤ä¸ª value çš„æƒ…å†µä¸ºä¾‹ï¼Œéœ€è¦å¾—åˆ°çš„ä¸­é—´é‡ $\boldsymbol{z}$ï¼ˆç†è§£ä¸ºæ³¨æ„åŠ›äº¦å¯ï¼‰å¯ä»¥é€šè¿‡ä¸‹å¼è®¡ç®—ï¼š

$$\begin{align}
    \boldsymbol{z}_1=\theta_{11}\boldsymbol{v}_1+\theta_{12}\boldsymbol{v}_2\\
    \boldsymbol{z}_2=\theta_{21}\boldsymbol{v}_1+\theta_{22}\boldsymbol{v}_2
\end{align}$$

æƒå€¼ $\theta_{ij}$ï¼ˆå³ä¸Šæ–‡æ‰€è¯´æ¦‚ç‡ï¼‰é€šè¿‡ä¸‹å¼å¾—åˆ°ï¼š

$$\theta_{ij}=\mathrm{Softmax}\left(\frac{\boldsymbol{q}_i\boldsymbol{k}^\top_j}{\sqrt{d_k}}\right)$$

å°†ä¸Šè¿°è¿ç®—è½¬ä¸ºçŸ©é˜µå½¢å¼ä¼šç®€æ´è®¸å¤šï¼š

$$
\begin{pmatrix}
    \boldsymbol{z}_1 \\
    \boldsymbol{z}_2
\end{pmatrix}=
\begin{pmatrix}
    \theta_{11} & \theta_{12} \\
    \theta_{21} & \theta_{22}
\end{pmatrix}
\begin{pmatrix}
    \boldsymbol{v}_1 \\
    \boldsymbol{v}_2
\end{pmatrix}\\
$$

å¯ä»¥è®°ä½œ $\boldsymbol{Z}=\boldsymbol{\theta}\boldsymbol{V}$ï¼Œä¹Ÿå°±æ˜¯

$$\mathrm{Attention}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = \mathrm{Softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^\top}{\sqrt{d_k}}\right)\boldsymbol{V}$$

#### Multi-Head Attention

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8803?authkey=ALYpzW-ZQ_VBXTU)

å‰ä¸€èŠ‚ä¸­è§£é‡Šäº† Transformer ä¸­çš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œä½†åœ¨æ¨¡å‹ä¸­å®é™…å¹¶éé€šè¿‡ä¸Šè¿°æ–¹å¼ç›´æ¥è®¡ç®—ï¼Œè€Œæ˜¯é€šè¿‡å¤šå¤´æ³¨æ„åŠ›çš„æ–¹å¼è®¡ç®—æ³¨æ„åŠ›ã€‚

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå¤šå¤´æ³¨æ„åŠ›åŒæ ·æ˜¯åœ¨è®¡ç®—ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œä½†ä¸çº¯ç²¹ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›çš„ä¸åŒä¹‹å¤„åœ¨äºå¤šå¤´æ³¨æ„åŠ›å°†å¤šä¸ªæ³¨æ„åŠ›è®¡ç®—æ­¥éª¤å åŠ äº†èµ·æ¥ã€‚

å åŠ çš„æ¬¡æ•°ä¸º $h$ï¼Œå³ä»£è¡¨ headï¼Œå¤šå°‘ä¸ª head è¡¨ç¤ºéœ€è¦è¿›è¡Œå¤šå°‘æ¬¡å åŠ è®¡ç®—ã€‚çŸ©é˜µ $\boldsymbol{Q}$ã€$\boldsymbol{K}$ã€$\boldsymbol{V}$ è¿›å…¥å¤šå¤´æ³¨æ„åŠ›è®¡ç®—æ­¥éª¤åï¼Œé¦–å…ˆè¦åˆ†åˆ«åœ¨ç¬¬ $i$ ä¸ª head ä¸­è¿›è¡Œçº¿æ€§å˜æ¢å¹¶è®¡ç®—æ³¨æ„åŠ›ï¼š

$$\mathrm{head}_i=\mathrm{Attention}(\boldsymbol{Q}\boldsymbol{W}^Q_i,\boldsymbol{K}\boldsymbol{W}^K_i,\boldsymbol{V}\boldsymbol{W}^V_i)$$

å…¶ä¸­ $\boldsymbol{W}^Q_i\in\mathbb{R}^{d_\mathrm{model}\times d_k}$ï¼Œ$\boldsymbol{W}^K_i\in\mathbb{R}^{d_\mathrm{model}\times d_k}$ï¼Œ$\boldsymbol{W}^V_i\in\mathbb{R}^{d_\mathrm{model}\times d_v}$ï¼Œæ³¨æ„ä¸åŒ head ä¸­çš„çº¿æ€§å˜æ¢å¹¶ä¸åŒï¼Œè¾“å‡ºä¹Ÿä¸åŒã€‚ç„¶åå°†æ‰€æœ‰è¾“å‡º $\mathrm{head}_i$ æ‹¼åˆåœ¨ä¸€èµ·ï¼Œç»çº¿æ€§å˜æ¢åä½œä¸ºæ³¨æ„åŠ›ï¼š

$$\mathrm{MultiHead}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})=\mathrm{Concat}(\mathrm{head}_1,\mathrm{head}_2,\cdots,\mathrm{head}_h)\boldsymbol{W}^O$$

å…¶ä¸­ $\boldsymbol{W}^O\in\mathbb{R}^{hd_v\times d_\mathrm{model}}$ã€‚

æ³¨æ„è¿™ä¸ªè¿‡ç¨‹ä¸­æ•°æ®ç»´æ•°çš„å˜åŒ– $d_\mathrm{model}$ ä¸ºå•å¤´æ³¨æ„åŠ›ä¸­æ¨¡å‹æ‰€å¤„ç†çš„ç»´æ•°ï¼Œ$\boldsymbol{W}^Q_i$ï¼Œ$\boldsymbol{W}^K_i$ï¼Œ$\boldsymbol{W}^V_i$ çš„çº¿æ€§å˜æ¢å°† queryã€key çš„ç»´æ•°ä» $d_\mathrm{model}$ æå‡åˆ° $d_v$ï¼Œå°† value çš„ç»´æ•°ä» $d_\mathrm{model}$ æå‡è‡³ $d_v$ã€‚æœ€åçš„ $\boldsymbol{W}^O$ åˆå°†æ‹¼åˆèµ·æ¥ç»´æ•°ä¸º $hd_v$ çš„æ³¨æ„åŠ›è½¬æ¢ä¸ºæ¨¡å‹æ‰€å¤„ç†çš„ç»´æ•° $d_\mathrm{model}$ã€‚è¿™äº›çº¿æ€§å˜æ¢çŸ©é˜µ $\boldsymbol{W}_i$ å®é™…ä¸Šå°±æ˜¯æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦å­¦ä¹ çš„ä¸€éƒ¨åˆ†å‚æ•°ã€‚

è‡³äºä¸ºä»€ä¹ˆè¦ç”¨å¤šå¤´çš„æ–¹å¼è®¡ç®—æ³¨æ„åŠ›ï¼Œè¿™å°±æ˜¯ä¸ªå¾ˆå¤æ‚çš„é—®é¢˜äº†ã€‚å°±æˆ‘çš„ç†è§£è€Œè¨€ï¼Œç”±äºæ¯ä¸ª head ä¸­çš„çº¿æ€§å˜æ¢çŸ©é˜µ $\boldsymbol{W}_i$ï¼Œå¤šå¤´æ³¨æ„åŠ›å®é™…ä¸Šæ˜¯å°† queryã€keyã€value æ˜ å°„åˆ°ä¸åŒçš„å­ç©ºé—´ä¸­ï¼Œåœ¨å¤šä¸ªä¸åŒçš„å­ç©ºé—´ä¸­å¯»æ‰¾ä¸ query æœ€åŒ¹é…çš„ key-valueã€‚ç”±äºä¸åŒå­ç©ºé—´ä¸­å…·æœ‰ä¸åŒæ–¹é¢çš„ä¿¡æ¯ï¼Œæœ€åå°†å…¶æ‹¼æ¥èµ·æ¥ä½œä¸ºç»“æœï¼Œè¿™æ ·å¯ä»¥æ›´å¤šåœ°ä»å¤šä¸ªæ–¹é¢æ•è·æ•°æ®ä¸­çš„ä¿¡æ¯ã€‚

#### Feed-Forward å±‚

åœ¨å¤šå¤´æ³¨æ„åŠ›å±‚ä¹‹åï¼Œå°±æ˜¯å‰é¦ˆå±‚ï¼Œå‰é¦ˆå±‚åªåœ¨ä½ç½®æ–¹å‘ä¸Šè®¡ç®—ï¼Œæ‰€ä»¥åŸæ–‡æè¿°å…¶ä¸º position-wiseã€‚è¿›å…¥å‰é¦ˆå±‚çš„æ•°æ®åœ¨è¯¥å±‚ä¸­å…ˆåš 1 æ¬¡çº¿æ€§å˜æ¢ï¼Œç»´åº¦å‡é«˜ï¼Œå†ç»è¿‡ RELU æ¿€æ´»å‡½æ•°ï¼Œæœ€åå†åš 1 æ¬¡çº¿æ€§å˜æ¢ï¼Œç»´åº¦é™ä½ï¼Œè¾“å…¥ä¸è¾“å‡ºå‰é¦ˆå±‚çš„ç»´åº¦ç›¸åŒã€‚ä¸Šè¿°è¿‡ç¨‹å¯ä»¥è¡¨ç¤ºä¸º

$$\mathrm{FFN}(\boldsymbol{x})=\max(0,\boldsymbol{x}\boldsymbol{W}_1+b_1)\boldsymbol{W}_2+b_2$$

RELU æ¿€æ´»å‡½æ•°å®šä¹‰ä¸º

$$\mathrm{ReLU}(x)=x^+=\max(0,x)$$

å³å¼ä¸­çš„ $\max$ï¼ŒæŒ‰åŸæ–‡ä¸­çš„ä¾‹å­ï¼Œ$\boldsymbol{W}_1$ ä½¿ $\boldsymbol{x}$ ç”± 512 ç»´å‡é«˜åˆ° 2048 ç»´ï¼Œ$\boldsymbol{W}_2$ ä½¿ $\boldsymbol{x}$ è®¡ç®—ç”± 2048 ç»´å†é™è‡³ 512 ç»´ï¼Œå‡ç»´ä¸é™ç»´çš„è¿‡ç¨‹ä¹Ÿæ˜¯ä¸ºäº†æ›´å¥½åœ°è·å¾—æ•°æ®ä¸­çš„ä¿¡æ¯ã€‚

### Transformer è®¡ç®—æ­¥éª¤

Transformer æ¨¡å‹å¤§è‡´å°±ç”±ä¸Šè¿°çš„å‡ ä¸ªå±‚è¿æ¥åœ¨ä¸€èµ·æ„æˆï¼Œä½†æˆ–è®¸è¿˜æ˜¯è§‰å¾—æœ¦æœ¦èƒ§èƒ§ï¼Œæ¯”å¦‚ç©¶ç«Ÿä»€ä¹ˆæ‰æ˜¯ queryã€keyã€value ç­‰ç­‰ã€‚ä¸å¦¨å†æ¥çœ‹çœ‹ Transformer çš„ç»“æ„å›¾ï¼Œè¿™ä¸€æ¬¡å·²ç†ŸçŸ¥å¤§éƒ¨åˆ†æ¨¡å—çš„å·¥ä½œåŸç†äº†ï¼Œæ‰€ä»¥åªçœ‹æ•°æ®æµå…¥ä¸æµå‡ºå„æ¨¡å—çš„è·¯çº¿ã€‚

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!7721?authkey=ALYpzW-ZQ_VBXTU)

ä½œä¸º source çš„ $\boldsymbol{X}$ ä¸ä½œä¸º target çš„ $\boldsymbol{Y}$ åˆ†åˆ«ä»ä¸‹æ–¹çš„å·¦å³ä¸¤ä¾§è¿›å…¥æ¨¡å‹ã€‚$\boldsymbol{X}$ ä¸ $\boldsymbol{Y}$ éƒ½è¦ç»è¿‡è¯åµŒå…¥å¹¶åŠ ä¸Šä½ç½®ç¼–ç ï¼ŒæŒ‰ä»¥ä¸‹æ–¹å¼æ›´æ–°ï¼š

$$
\begin{align}
    \boldsymbol{X}&\leftarrow\mathrm{Embedding}(\boldsymbol{X})+\mathrm{PE}(\boldsymbol{X})\\
    \boldsymbol{Y}&\leftarrow\mathrm{Embedding}(\boldsymbol{Y})+\mathrm{PE}(\boldsymbol{Y})
\end{align}
$$

æ¥ç€ $\boldsymbol{X}$ ä¸ $\boldsymbol{Y}$ åˆ†åˆ«è¿›å…¥ encoder ä¸ decoderï¼Œå¯ä»¥æ³¨æ„åˆ°æ•°æ®åˆ†ä½œ 4 æ¡è·¯çº¿ï¼Œè¿™æ„å‘³ç€å°†æ•°æ®å¤åˆ¶ 4 æ¬¡ã€‚å…ˆçœ‹è¿›å…¥å¤šå¤´æ³¨æ„åŠ›å±‚çš„ 3 æ¡æ•°æ®ï¼Œä»¥ encoder ä¸ºä¾‹ï¼Œåœ¨è¿™ä¸€å±‚ä¸­å°±æ˜¯åœ¨è®¡ç®—

$$\mathrm{Attention}(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X})$$

ä¸è¨€è‡ªæ˜ï¼Œåœ¨è¿™é‡Œçš„ queryã€keyã€value ä¸‰è€…éƒ½æ˜¯ $\boldsymbol{X}$ï¼Œæ˜¯åœ¨ $\boldsymbol{X}$ å†…éƒ¨è®¡ç®—æ³¨æ„åŠ›ï¼Œå› æ­¤ç§°å…¶ä¸º**è‡ªæ³¨æ„åŠ›**ï¼ˆself-attentionï¼‰ã€‚

åœ¨åç»­çš„ Add & Norm å±‚ä¸­ï¼Œè®¡ç®—

$$\boldsymbol{X}\leftarrow\mathrm{LayerNorm}(\boldsymbol{X}+\mathrm{Attention}(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X}))$$

åœ¨å‰é¦ˆå±‚ä¸åç»­çš„ Add & Norm å±‚è¾“çš„è¾“å‡ºç»“æœä¹Ÿå¯æƒ³è€ŒçŸ¥ï¼š

$$\boldsymbol{X}\leftarrow\mathrm{LayerNorm}(\boldsymbol{X}+\max(0,\boldsymbol{X}\boldsymbol{W}_1+b_1)\boldsymbol{W}_2+b_2)$$

è¿™é‡Œçš„ $\boldsymbol{X}$ åˆ†ä½œä¸¤è·¯è¿›å…¥åˆ° decoder ä¸­ï¼Œåœ¨ decoder çš„è¯¥å¤šå¤´æ³¨æ„åŠ›å±‚ä¸­ï¼Œquery ä¸ key ä¸º $\boldsymbol{X}$ï¼Œè€Œ value ä¸ºç±»ä¼¼æ­¥éª¤å¾—åˆ°çš„ $\boldsymbol{Y}$ï¼Œè¯¥å±‚çš„è¾“å‡ºä¸º

$$\boldsymbol{Z}=\mathrm{Attention}(\boldsymbol{X},\boldsymbol{X},\boldsymbol{Y})$$

è¿™ä¹Ÿæ˜¯ decoder ä¸ encoder çš„å…³é”®ä¸åŒã€‚è¾“å‡ºç»“æœ $\boldsymbol{Z}$ å®Œæˆåç»­çš„è®¡ç®—è¿‡ç¨‹åï¼Œå°±å¾—åˆ°å„ token çš„æ¦‚ç‡ï¼Œç”¨å„ token æ›¿æ¢å³å¯å¾—åˆ°æ¨¡å‹è¾“å‡ºçš„æ–‡æœ¬ç»“æœã€‚

{note begin}æœ‰å…´è¶£çš„è¯»è€…ä¸å¦¨æ ¹æ®å„çŸ©é˜µçš„å½¢çŠ¶å°è¯•è®¡ç®—ä¸€ä¸‹å„ä¸ªå˜é‡çš„ç»´åº¦åœ¨ Transformer åœ¨å„æ­¥éª¤ä¸­æ˜¯å¦‚ä½•å˜åŒ–çš„ï¼Œä¸€å®šä¼šå¯¹ Transformer çš„è®¡ç®—è¿‡ç¨‹æ”¶è·æ›´æ·±çš„äº†è§£ã€‚{note end}

## ä»£ç æ‹†è§£

æœ‰äº†å¯¹ Transformer åŸç†çš„åŸºæœ¬è®¤è¯†ï¼Œå°±å¯ä»¥åŠ¨æ‰‹å®ç°ä¸€ä¸ª Transformer äº†ï¼Œé€šè¿‡ä»£ç æ›´æ·±å…¥äº†è§£ Transformer çš„ä¸€äº›ç»†èŠ‚ã€‚è¿™é‡Œä½¿ç”¨ PyTorch æ­å»ºä¸€ä¸ªæ ‡å‡†çš„ Transformerï¼Œå‚è€ƒä»£ç è§ [<i class="fa fa-github"></i> aladdinpersson / Machine-Learning-Collection ](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/transformer_from_scratch/transformer_from_scratch.py)ã€‚

ä»£ç ä¸­çš„å„æ¨¡å—å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ¥ä¸‹æ¥å¯¹å„æ¨¡å—é€ä¸ªæ‹†è§£ã€‚

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8825?authkey=ALYpzW-ZQ_VBXTU)

### PositionEmbedding

```py
import math
import torch
import torch.nn as nn


class PositionEmbedding(nn.Module):
    def __init__(self, d_model, max_len=1000):
        # d_model ä¸ºæ¨¡å‹å¤„ç†æ•°æ®çš„ç»´æ•°ï¼Œå³å…¬å¼ä¸­ d_k
        # max_len è¡¨ç¤ºæ¨¡å‹å¤„ç†çš„æœ€å¤§ token æ•°é‡
        super(PositionEmbedding, self).__init__()

        # ç”Ÿæˆå¤§å°ä¸º max_len * d_model çš„é›¶çŸ©é˜µ
        pe = torch.zeros(max_len, d_model)
        # ç”Ÿæˆå¤§å°ä¸º max_len * 1 çš„ä½ç½®çŸ©é˜µ
        position = torch.arange(max_len).unsqueeze(1)
        # è®¡ç®—ä½ç½®ç¼–ç 
        div_term = torch.exp(torch.arange(0, d_model, 2) * - (math.log(10000.0) / d_model))
        x = position * div_term
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = self.pe[:, :x.size(1)]
        return x
```

é¦–å…ˆå®ç°ä½ç½®ç¼–ç æ¨¡å—ã€‚åœ¨ PyTorch ä¸­ï¼Œç”¨äºæ­å»ºç¥ç»ç½‘ç»œçš„æ¨¡å—éƒ½è¦ç»§æ‰¿ `nn.Module`ï¼ŒPyTorch ä¼šé€šè¿‡ `__call__()` è°ƒç”¨æ¨¡å—çš„ `forward()` çš„æ–¹æ³•è¿›è¡Œå‰å‘ä¼ æ’­ã€‚ç®€å•æ¥è®²å°±æ˜¯ï¼Œ`PositionEmbedding(x)` çš„åŠŸèƒ½ç­‰åŒäº `PositionEmbedding.forward(x)`ï¼Œä½†ä¸èƒ½ä½¿ç”¨ `PositionEmbedding.forward(x)`ï¼Œå› ä¸º PyTorch åšäº†è®¸å¤šæ¡ä»¶çš„åˆ¤å®šå’Œä¼˜åŒ–ã€‚

`torch.arange(num)` çš„åŠŸèƒ½ç±»ä¼¼äº Python ä¸­çš„ `range(num)`ï¼Œç”¨äºç”Ÿæˆæ–‡æœ¬å„ token çš„é¡ºåºä½ç½®ç´¢å¼•ã€‚`unsqueeze(dim)` ä¼šä»¤ Tensor åœ¨æŒ‡å®šçš„ç»´åº¦ `dim` ä¸Šæ‰©å¼  1 ç»´ï¼Œè¿™é‡Œæ˜¯ä¸ºäº†ä½¿ `pe` ä¸ `position` ä¸¤ä¸ªçŸ©é˜µçš„ç»´åº¦å¯¹é½ï¼Œä¾‹å¦‚ï¼š

```python-repl
>>> torch.arange(5)
tensor([0, 1, 2, 3, 4])
>>> torch.arange(5).unsqueeze(0)
tensor([[0, 1, 2, 3, 4]])
>>> torch.arange(5).unsqueeze(1)
tensor([[0],
        [1],
        [2],
        [3],
        [4]])
>>> torch.arange(5).size()
torch.Size([5])
>>> torch.arange(5).unsqueeze(0).size()
torch.Size([1, 5])
>>> torch.arange(5).unsqueeze(1).size()
torch.Size([5, 1])
```

ä»£ç ä¸­çš„ä½ç½®ç¼–ç å¹¶ä¸æ˜¯ç›´æ¥æŒ‰å…¬å¼è®¡ç®—çš„ï¼Œè€Œæ˜¯åšäº†ä¸€äº›å˜æ¢ï¼Œå…ˆè®¡ç®—ä¸€ä¸ªä¸­é—´é‡ `div_term`ï¼Œå…¶ä¸­ `torch.arange(0, d_model, 2)` å³ä¸º $2i$ï¼Œå¯ä»¥æ•´ç†å‡º

$$\begin{align}
    \mathrm{div\_term}_i&=\exp\left[2i\times(-\frac{\ln10000}{d_k})\right]\\
    &=\left[\exp(-\frac{\ln10000}{d_k})\right]^{2i}\\
    &=\left[10000^{-\frac{1}{d_k}}\right]^{2i}\\
    &=10000^{-2i/d_k}
\end{align}
$$

æ‰€ä»¥ `position * div_term` å°±å¯ä»¥å¾—åˆ°

$$\mathrm{position}\times \mathrm{div\_term}_i=\mathrm{pos}/10000^{2i/d_k}$$

å°±æ˜¯ä½ç½®ç¼–ç ä¸­çš„ä¸€é¡¹ã€‚

`pe[:, 0::2]` ä¸ `pe[:, 1::2]` æ˜¯ Pytorch ä¸­çš„é«˜çº§ç´¢å¼•æ“ä½œã€‚ç´¢å¼•ä¸­ç”¨ `,` åˆ†éš”ä¸åŒç»´åº¦ï¼Œä¾‹ä¸­ä»¥ `,` ä¸ºåˆ†ç•Œï¼Œå‰é¢æ˜¯å¯¹ç¬¬ 1 ç»´çš„ç´¢å¼•ï¼Œåé¢æ˜¯å¯¹ç¬¬ 2 ç»´çš„ç´¢å¼•ã€‚ç´¢å¼•æ“ä½œä¹Ÿéµå®ˆ Python çš„è§„åˆ™ï¼Œå³ `a:b:c` ä¸­ `a` ä¸ºèµ·å§‹ï¼Œ`b` ä¸ºæœ«å°¾ï¼Œ`c` ä¸ºæ­¥é•¿ã€‚

æ‰€ä»¥ `pe[:, 0::2]` ä¸ `pe[:, 1::2]` å–å‡ºå…¨éƒ¨ç¬¬ 1 ç»´ä¸­çš„å…ƒç´ ï¼Œå³è¡Œæ–¹å‘ä¸Šä¸æ“ä½œï¼Œå†åœ¨ç¬¬ 2 ç»´ä¸­åˆ†åˆ«ä» `0` æˆ– `1` å¼€å§‹ä»¥æ­¥é•¿ `2` å–å‡ºå…ƒç´ ï¼Œå³å–å‡ºç¬¬ $2i$ æˆ–ç¬¬ $2i+1$ åˆ—ã€‚

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8811?authkey=ALYpzW-ZQ_VBXTU)

åœ¨ `forward()` éƒ¨åˆ†ï¼Œè¾“å‡ºçš„ä½ç½®ç¼–ç ä¸º `pe[:, :x.size(1)]`ï¼Œè¿™ä¸»è¦æ˜¯ä¸ºäº†ç¡®ä¿çŸ©é˜µå½¢çŠ¶åœ¨åŠ æ³•è¿‡ç¨‹ä¸­ä¸ä¼šå› éæ³•è¾“å…¥çš„å¹¿æ’­è€Œæ”¹å˜ã€‚å…¶å®åœ¨è¾“å…¥åˆæ³•çš„æƒ…å†µä¸‹ï¼Œ`x.size(1)` å°±æ˜¯ `d_model`ï¼Œç­‰ä»·äº `pe[:, :]`ï¼Œä¹Ÿç­‰ä»·äº `pe`ã€‚

<!-- æŒ‡å®š `requires_grad_(False)` æ˜¯å› ä¸º PyTorch ä¼šè‡ªåŠ¨ä¿å­˜ Tensor çš„æ¥æºï¼Œç”¨äºæ›´å¿«åœ°è®¡ç®—æ¢¯åº¦ï¼Œè€Œè¿™é‡Œçš„åŠ æ³•è®¡ç®—å¹¶ä¸æ˜¯è®­ç»ƒè¿‡ç¨‹ï¼Œå–æ¶ˆä¿å­˜èƒ½èŠ‚çœä¸€éƒ¨åˆ†èµ„æºã€‚ -->

### SelfAttention

åœ¨è¿›å…¥ Transformer æ ¸å¿ƒéƒ¨åˆ†ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å†æ¬¡æ˜ç¡®ä¸€ä¸‹è¾“å…¥æ¨¡å‹çš„æ•°æ®æ ¼å¼ã€‚ä¸Šæ–‡ä¸­ä»…ä»¥è¾“å…¥æ¨¡å‹ä¸€æ¡æ•°æ®ï¼ˆç”±è‹¥å¹² token ç»„æˆçš„ä¸€æ¡å¥å­ï¼‰ä¸ºä¾‹ï¼Œåœ¨å®é™…æ“ä½œä¸­ï¼Œä¸ºäº†æé«˜è®­ç»ƒæ•ˆç‡ï¼Œä¼šåŒæ—¶è¾“å…¥è‹¥å¹²æ¡æ•°æ®ï¼Œåœ¨æ„å»ºæ¨¡å‹æ—¶ä¹Ÿè¦è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ã€‚

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8810?authkey=ALYpzW-ZQ_VBXTU)

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œä¸€æ¬¡è¾“å…¥æ¨¡å‹çš„æ•°æ®æ¡æ•°å°±ç§°ä¸º batch sizeï¼Œæ‰€ä»¥æ¨¡å‹æ‰€å¤„ç†çš„å…¶å®æ˜¯ä¸€ä¸ª $\mathrm{batch\_size}\times\mathrm{max\_len}\times\mathrm{d\_model}$ çš„é«˜ç»´çŸ©é˜µã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œ`x.size()` çš„ç»“æœæ˜¯ `[batch_size, max_len, d_model]`ï¼ŒåŠ¡å¿…æ³¨æ„ä¸‰è€…é¡ºåºã€‚

```py
class SelfAttention(nn.Module):
    def __init__(self, embed_size, heads):
        super(SelfAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads
        # ç¡®ä¿ embed_size èƒ½è¢« heads æ•´é™¤
        assert (
            self.head_dim * heads == embed_size
        ), "Embedding size needs to be divisible by heads"

        self.values = nn.Linear(embed_size, embed_size)
        self.keys = nn.Linear(embed_size, embed_size)
        self.queries = nn.Linear(embed_size, embed_size)
        self.fc_out = nn.Linear(embed_size, embed_size)
```

å…ˆçœ‹ `SelfAttention` çš„åˆå§‹åŒ–éƒ¨åˆ†ï¼Œæ˜ç™½äº†æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—è¿‡ç¨‹å°±ä¸éš¾ç†è§£ä¸Šé¢çš„å„ä¸ªå±æ€§äº†ã€‚`head_dim` æ˜¯æ¯ä¸€ä¸ª head ä¸­æ³¨æ„åŠ›çš„ç»´åº¦ï¼Œ`embeds_size` å¿…é¡»èƒ½è¢« `heads` æ•´é™¤ï¼Œå¦åˆ™å°†å¤šå¤´æ³¨æ„åŠ›æ‹¼æ¥åœ¨ä¸€èµ·çš„ç»´æ•°ä¸ç­‰äºæ¨¡å‹å¤„ç†çš„ç»´æ•°å°±ä¼šå‡ºç°é—®é¢˜ã€‚

`values`ã€`keys`ã€`queries` éƒ½æ˜¯è®¡ç®—å¤šå¤´æ³¨æ„åŠ›å‰çš„çº¿æ€§å˜æ¢ï¼Œ`fc_out` æ˜¯æ‹¼æ¥å¤šå¤´æ³¨æ„åŠ›åçš„çº¿æ€§å˜æ¢ã€‚çº¿æ€§å˜æ¢å¯ä»¥ç›´æ¥è°ƒç”¨ `nn.Linear(in_dim, out_dim)`ï¼Œåªéœ€è¦æŒ‡å®šçº¿æ€§å˜æ¢å‰åçš„ç»´æ•°å³å¯ï¼Œè¿™é‡Œçº¿æ€§å˜æ¢å‰åç»´æ•°æ²¡æœ‰å˜åŒ–ã€‚

å¯èƒ½ä¼šæœ‰è¯»è€…ç–‘æƒ‘ä¸ºä»€ä¹ˆè¿™é‡Œæ‰€è®¾å®šçš„çº¿æ€§å˜æ¢ä¸æ”¹å˜ç»´æ•°ï¼ŒåŸæ–‡ä¸­æ‰€æè¿°çš„æ­¥éª¤ä¸æ˜¯åº”è¯¥å°† $d_\mathrm{model}$ å‡è‡³ $d_v$ å†è®¡ç®—æ³¨æ„åŠ›å—ï¼Ÿè¿™æ˜¯æ­£ç¡®çš„ï¼ŒåŸæ–‡ä¸­çš„è®¡ç®—æµç¨‹ç¡®å®å¦‚æ­¤ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåœ¨çº¿æ€§å˜æ¢åå¤åˆ¶ `h` ä»½ï¼ˆä¾‹ä¸­ä¸º 2ï¼‰ $\boldsymbol{Q}$ï¼Œç”¨è‹¥å¹²ä»½ $\boldsymbol{Q}$ åˆ†åˆ«è®¡ç®—æ³¨æ„åŠ›å†æ‹¼åˆèµ·æ¥ï¼Œå¾—åˆ°æ³¨æ„åŠ›çš„ç»´æ•°è‡ªç„¶å°±æ˜¯ `h * d_v` ï¼ˆä¾‹ä¸­ä¸º 2 * 6ï¼‰ï¼Œå†ç”¨ä¸€ä¸ªçº¿æ€§å˜æ¢å°†å…¶è½¬åŒ–å›æ¨¡å‹æ‰€å¤„ç†çš„ç»´æ•° `d_model`ï¼ˆä¾‹ä¸­ä¸º 5ï¼‰ã€‚

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8812?authkey=ALYpzW-ZQ_VBXTU)

ä½†ä»£ç ä¸­ä¼˜åŒ–äº†ä¸€éƒ¨åˆ†æ¯”è¾ƒç¹ççš„æ“ä½œï¼Œä¹Ÿæœ‰å…¶ä»–ç‰ˆæœ¬çš„ä»£ç ä½¿ç”¨äº†æ›´æ¥è¿‘åŸæ–‡çš„å®ç°æ–¹å¼ï¼Œå¦‚  [<i class="fa fa-github"></i> jadore801120 / attention-is-all-you-need-pytorch ](https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/SubLayers.py)ï¼Œæµç¨‹å°±å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå‹‰å¼ºç§°ä¹‹ä¸ºã€Œå•å¤´æ³¨æ„åŠ›å˜å¤šå¤´æ³¨æ„åŠ›ã€çš„ä¸€ç§ä»£ç å®ç°å§ã€‚

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8814?authkey=ALYpzW-ZQ_VBXTU)

ä¾‹ä¸­ `d_model` ä¹Ÿå°±æ˜¯è¯åµŒå…¥çš„ç»´æ•°è¿˜æ˜¯ 5ï¼Œ`heads` ä»ä¸º 2ï¼Œ`d_value` ä»ä¸º 6ï¼Œä½†æ¨¡å‹ä¸å†æ˜¯å°† $d_\mathrm{model}$ å‡è‡³ $d_v$ï¼Œè€Œæ˜¯å°† $d_\mathrm{model}$ ç›´æ¥å‡è‡³ $hd_v$ï¼Œç„¶åå°† $\boldsymbol{Q}$ åˆ†æˆ `h` ä»½ï¼Œæ¯ä»½åˆ†åˆ«ç”¨äºè®¡ç®—å¹¶æ‹¼æ¥ä¸ºæ³¨æ„åŠ›ã€‚ä¸ä¸Šä¾‹ç›¸æ¯”ï¼Œæœ¬è´¨ä¸Šå…¶å®å¹¶æ— åŒºåˆ«ï¼ŒåŒºåˆ«ä»…ä»…æ˜¯ä¸Šä¾‹å…ˆå¤åˆ¶å¤šä¸ªçŸ©é˜µå†åˆ†åˆ«åšçº¿æ€§å˜æ¢ï¼Œè€Œè¯¥ä¾‹åªä½¿ç”¨äº†ä¸€ä¸ªæ›´å¤§çš„çŸ©é˜µä¹˜æ³•å°±å®Œæˆäº†ä¸Šè¿°æ“ä½œï¼Œæ•ˆç‡ä¸Šæ›´ä¼˜ã€‚

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8815?authkey=ALYpzW-ZQ_VBXTU)

å¤šå¤´æ³¨æ„åŠ›è¿˜æœ‰ä¸€ç§å®ç°æ–¹æ³•ï¼Œä¹Ÿæ˜¯è¿™é‡Œå±•ç¤ºä»£ç æ‰€ä½¿ç”¨çš„æ–¹æ³•ã€‚å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œè¿™ç§æ–¹æ³•å¯¹è¯åµŒå…¥çš„ç»´æ•°æœ‰è¦æ±‚ï¼Œåœ¨è¯åµŒå…¥çš„æ­¥éª¤ä¸­å°±å°† token è¡¨ç¤ºä¸º `d_v * h` ç»´ï¼Œè¿™ä¹Ÿæ˜¯å‰æ–‡ä»£ç åœ¨åˆå§‹åŒ–ä¸­ä½¿ç”¨ `assert` è¯­å¥ç¼˜ç”±ã€‚åç»­çš„çº¿æ€§å˜æ¢ä¸æ”¹å˜ç»´æ•°ï¼Œè®¡ç®—å¤šå¤´æ³¨æ„åŠ›æ—¶ç›´æ¥å°† `d_v * h` ç»´åˆ‡åˆ†ä¸º `h` ä»½ä½œä¸ºæ¯ä¸ª head è®¡ç®—çš„å¯¹è±¡ã€‚æ‹¼æ¥å„ head çš„æ³¨æ„åŠ›åï¼Œæœ€åçš„çº¿æ€§å˜æ¢ä¹Ÿä¸æ”¹å˜ç»´æ•°ã€‚

åœ¨æˆ‘çœ‹æ¥ï¼Œè¿™ç§æ–¹æ³•åº”è¯¥æ˜¯å¯¹å‰ä¸¤ç§æ–¹æ³•çš„ç®€åŒ–ï¼Œä¸‰ä¸ªä¾‹å­ä¸­ç”¨äºè®¡ç®—å¤šå¤´æ³¨æ„åŠ›çš„ `d_value` éƒ½ä¸º 6ï¼Œè®¡ç®—é‡ç›¸åŒã€‚ç¬¬ 3 ç§æ–¹æ³•éœ€è¦æ›´å¤§çš„ `d_model`ï¼Œè€Œä¸”è®¡ç®—å¤šå¤´æ³¨æ„åŠ›æ—¶æ²¡æœ‰ä½¿ç”¨åˆ°å…¨éƒ¨çš„ embeddingï¼Œè™½è¯´æ•ˆæœç±»ä¼¼ï¼Œä½†æ€»è§‰æœ‰äº›å¥‡æ€ªã€‚è¿™æˆ–è®¸æ˜¯ä¸ºäº†è®¡ç®—ä¸Šçš„æ–¹ä¾¿ï¼Œä¸ç”¨åšè¿‡å¤šçš„çŸ©é˜µå˜æ¢ ğŸ¤”

```py
# class SelfAttention(nn.Module):
    def forward(self, values, keys, query, mask):
        # è·å– batch_size
        N = query.shape[0]
        # d_v, d_k, d_q
        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]

        # å¯¹ query, key, value åšçº¿æ€§å˜æ¢
        values = self.values(values)    # (N, value_len, embed_size)
        keys = self.keys(keys)          # (N, key_len, embed_size)
        queries = self.queries(query)   # (N, query_len, embed_size)

        # å°† token çš„è¯åµŒå…¥åˆ’åˆ†ä¸º heads ä»½
        # d_model = embed_size = d_v * heads
        values = values.reshape(N, value_len, self.heads, self.head_dim)
        keys = keys.reshape(N, key_len, self.heads, self.head_dim)
        queries = queries.reshape(N, query_len, self.heads, self.head_dim)

        # queries: (N, query_len, heads, heads_dim),
        # keys: (N, key_len, heads, heads_dim)
        # energy: (N, heads, query_len, key_len)
        energy = torch.einsum("nqhd,nkhd->nhqk", [queries, keys])

        # å°†æ©ç çŸ©é˜µä¸­ä¸º 0 çš„å¯¹åº”é¡¹è®¾ä¸º -infï¼Œä¸å‚ä¸è®¡ç®—
        if mask is not None:
            energy = energy.masked_fill(mask == 0, float("-1e20"))

        # å¾—åˆ°çš„ç‚¹ç§¯é™¤ä»¥ sqrt(d_k) å¹¶ç”¨ Softmax å½’ä¸€åŒ–
        # attention: (N, heads, query_len, key_len)
        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)

        # attention: (N, heads, query_len, key_len)
        # values: (N, value_len, heads, heads_dim)
        # out after matrix multiply: (N, query_len, heads, head_dim), then
        # we reshape and flatten the last two dimensions.
        out = torch.einsum("nhql,nlhd->nqhd", [attention, values]).reshape(
            N, query_len, self.heads * self.head_dim
        )

        # æ‹¼æ¥å¤šå¤´æ³¨æ„åŠ›åçš„çº¿æ€§å˜æ¢
        # out: (N, query_len, embed_size)
        out = self.fc_out(out)

        return out
```

`forward()` éƒ¨åˆ†æè¿°äº†ä¸Šè¿°è®¡ç®—å¤šå¤´é‡æ„åŠ›çš„è¿‡ç¨‹ã€‚çº¿æ€§å˜æ¢åï¼Œä½¿ç”¨ `reshape()` æ–¹æ³•å°† Tensor è½¬åŒ–åŒ–ä¸ºæŒ‡å®šç»´åº¦ï¼Œä¹Ÿå°±æ˜¯å°†è¯åµŒå…¥åˆ’åˆ†ä¸º `heads` ä»½çš„æ“ä½œï¼ŒTensor çš„å½¢çŠ¶ç”± `[N, query_len, embed_size]` å˜ä¸º `[N, query_len, self.heads, self.head_dim]`ï¼ŒæŠŠ `embed_size` æ‹†æˆ `heads * head_dim`ã€‚

æ¥ç€ä½¿ç”¨ `torch.einsum()` å¾—åˆ°æ³¨æ„åŠ›è®¡ç®—çš„ä¸€ä¸ªä¸­é—´é‡ `energy`ã€‚`torch.einsum()` ç§°ä¸ºçˆ±å› æ–¯å¦æ±‚å’Œçº¦å®šï¼Œå¯ä»¥éå¸¸ç®€æ´åœ°è¿›è¡ŒçŸ©é˜µä¹˜æ³•ã€è½¬ç½®å¾…æ“ä½œï¼Œä½†ä¼šæœ‰äº›éš¾ä»¥ç†è§£ã€‚

ä¾‹å¦‚çŸ©é˜µä¹˜æ³• $\boldsymbol{A}_{i\times j}\boldsymbol{B}_{j\times k}=\boldsymbol{C}_{i\times k}$ï¼Œå¯ä»¥è¡¨ç¤ºä¸º `"ij,jk->ik"`ï¼š

```python-repl
>>> A = torch.randn(3, 4)
>>> B = torch.randn(4, 5)
>>> C = torch.einsum("ij,jk->ik", [A, B])
>>> C.size()
torch.Size([3, 5])
```

ä¾‹å¦‚çŸ©é˜µè½¬ç½® $(\boldsymbol{A}_{i\times j})^\top=\boldsymbol{B}_{j\times i}$ï¼Œå¯ä»¥è¡¨ç¤ºä¸º `"ij->ji"`ï¼š

```python-repl
>>> A = torch.randn(3, 4)
>>> B = torch.einsum("ij->ji", [A])
>>> B.size()
torch.Size([4, 3])
```

å®šä¹‰äº†çŸ©é˜µä¹˜æ³•çš„è¡¨ç¤ºåï¼Œç›¸åº”çš„æ•°é‡ç§¯ä¸å‘é‡ç§¯å°±ä¹Ÿèƒ½è¡¨ç¤ºäº†ï¼Œä¸å†èµ˜è¿°ã€‚æ±‚å’Œæ“ä½œå°†çŸ©é˜µè½¬åŒ–ä¸ºæ•°å€¼ï¼Œè¡Œä¸åˆ—éƒ½ä¼šæ¶ˆå¤±ï¼Œæ‰€ä»¥ $\sum a_{ij}\in\boldsymbol{A}_{i\times j}$ å¯ä»¥è®°ä½œ `"ij->"`ï¼š

```python-repl
>>> A = torch.randn(3, 4)
>>> torch.einsum("ij->", [A])
tensor(0.5634)
```

æ­¤å¤–ï¼Œçˆ±å› æ–¯å¦æ±‚å’Œçº¦å®šè¿˜å¯ä»¥è¡¨ç¤ºåœ¨æŒ‡å®šç»´åº¦ä¸Šæ±‚å’Œã€åšæ•°é‡ç§¯ç­‰ä¸€ç³»åˆ—çš„å¤æ‚æ“ä½œï¼Œè¯»è€…å¯ä»¥è‡ªè¡Œè¯•éªŒã€‚

ä»£ç ä¸­ `queries` çš„å½¢çŠ¶ä¸º `[N, query_len, heads, heads_dim]`ï¼Œè®°ä½œ $\boldsymbol{Q}_{N\times q\times h \times d}$ï¼Œ`keys` çš„å½¢çŠ¶ä¸º `[N, key_len, heads, heads_dim]`ï¼Œè®°ä½œ $\boldsymbol{K}_{N\times k\times h \times d}$ï¼Œé‚£ä¹ˆ `torch.einsum("nqhd,nkhd->nhqk", [queries, keys])` æ‰€åšçš„æ“ä½œå°±æ˜¯ï¼š

1. å°† $\boldsymbol{Q}_{N\times q\times h \times d}$ è½¬ç½®ä¸º $\boldsymbol{Q}_{N\times h \times q\times d}$ï¼Œå°† $\boldsymbol{K}_{N\times k\times h \times d}$ è½¬ç½®ä¸º $\boldsymbol{K}_{N\times h\times k \times d}$ï¼›
2. ä¸¤ä¸ªçŸ©é˜µä¸­çš„ $N\times h$ æ˜¯ `batch_size` ä¸ `heads` çš„ä¹˜ç§¯ï¼Œä»…ä»…æ˜¯è¡¨ç¤ºæ•°é‡ï¼Œæ‰€ä»¥ $\boldsymbol{K}_{N\times h \times k\times d}$ å¯ä»¥è§†ä½œç”± $N\times h$ ä¸ª $(\boldsymbol{K}_i)_{\ k\times d}$ å­çŸ©é˜µæ„æˆçš„å¤§çŸ©é˜µã€‚é‚£ä¹ˆå›ºå®šå‰ä¸¤ç»´ä¸å˜ï¼Œè½¬ç½®åä¸¤ç»´ï¼Œç›¸å½“äº**è½¬ç½®**æ‰€æœ‰å­çŸ©é˜µï¼Œå¾—åˆ° $\boldsymbol{K}_{N\times h \times d\times k}$ï¼›
3. å›ºå®šå‰ä¸¤ç»´ï¼Œä»¤ $\boldsymbol{Q}_{N\times h \times q\times d}$ ä¸ $\boldsymbol{K}_{N\times h \times d\times k}$ åœ¨åä¸¤ç»´ä¸Šåšä¹˜æ³•ï¼Œå¾—åˆ° $(\boldsymbol{QK})_{N\times h \times q \times k}$ã€‚

ä»”ç»†æ€è€ƒä¸Šè¿°çš„è½¬ç½®å’Œä¹˜æ³•è¿‡ç¨‹ï¼Œå®é™…ä¸Šå°±æ˜¯åœ¨åšå¤šå¤´æ³¨æ„åŠ›ä¸­çš„ $\boldsymbol{Q}\boldsymbol{K}^\top$ã€‚

æ©ç éƒ¨åˆ†çš„æ“ä½œå…ˆç•¥è¿‡ã€‚æ¥ç€ `torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)` å…ˆå°†å‰ä¸€æ­¥ä¸­å¾—åˆ° `energy` é™¤ä»¥ $\sqrt{d_k}$ å†ç”¨ Softmax å½’ä¸€åŒ–ã€‚æŒ‡å®šçš„ `dim=3` ä¸ `dim=-1` ç­‰ä»·ï¼Œå…¶ç›®çš„æ˜¯åœ¨æœ€åä¸€ç»´çš„æ–¹å‘ä¸Šå½’ä¸€åŒ–ã€‚

ä»¥ä¸€ä¸ªç®€å•çš„ $\boldsymbol{Q}\boldsymbol{K}^\top$ ä¹˜æ³•ä¸ºä¾‹ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œ$\boldsymbol{Q}$ ä¸ $\boldsymbol{K}$ çš„æ¯ä¸€è¡Œéƒ½æ˜¯ä¸€ä¸ª token çš„è¯åµŒå…¥è¡¨ç¤ºã€‚è®¡ç®—å¾—åˆ° $\boldsymbol{Q}\boldsymbol{K}^\top$ åéœ€è¦å½’ä¸€åŒ–ï¼Œ`softmax(dim=0)` æ˜¯åœ¨è¡Œæ–¹å‘ä¸Šå½’ä¸€åŒ–ï¼Œåœ¨å¾—åˆ°çš„ç»“æœä¸­ï¼Œå…¨éƒ¨è¡ŒåŠ èµ·æ¥ï¼Œå„å…ƒç´ ä¸º 1ï¼›`softmax(dim=1)` æ˜¯åœ¨åˆ—æ–¹å‘ä¸Šå½’ä¸€åŒ–ï¼Œç»“æœä¸­çš„å…¨éƒ¨åˆ—åŠ èµ·æ¥ï¼Œå„å…ƒç´ ä¸º 1ã€‚

è®¡ç®—æ³¨æ„åŠ›è¿˜æ˜¯ä¸ºäº†å¾—åˆ°æ›´å‡†ç¡®çš„ token è¡¨ç¤ºï¼Œæ‰€ä»¥å½’ä¸€åŒ–çš„æ–¹å‘åº”è¯¥ä¸åŸå§‹çš„ $\boldsymbol{Q}$ æ–¹å‘ç›¸åŒï¼Œå³ `softmax(dim=1)`ã€‚ä»£ç ä¸­ä¹Ÿæ˜¯ä¸€æ ·ï¼Œ$(\boldsymbol{QK})_{N\times h \times q \times k}$ æ˜¯ $N\times h$ ä¸ª $(\boldsymbol{Q}\boldsymbol{K}_i)_{q\times k}$ å­çŸ©é˜µï¼Œè¦åœ¨æ‰€æœ‰å­çŸ©é˜µçš„åˆ—æ–¹å‘ä¸Šåšå½’ä¸€åŒ–ï¼Œé‚£ä¹ˆå°±æ˜¯åœ¨ç¬¬ 4 ä¸ªç»´åº¦ä¸Šåš Softmaxï¼Œå³ `softmax(dim=3)`ã€‚

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8820?authkey=ALYpzW-ZQ_VBXTU)

æ­¤æ—¶ï¼Œä¸Šè¿°è¿‡ç¨‹å·²ç»å®Œæˆäº†å¤šå¤´æ³¨æ„åŠ›ä¸­çš„ $\mathrm{Softmax}(\boldsymbol{Q}\boldsymbol{K}^\top/\sqrt{d_k})$ï¼Œå°†ç»“æœè®°ä½œ $\boldsymbol{A}_{N\times h\times q\times k}$ã€‚

åœ¨ä¸‹ä¸€æ­¥ä¸­ï¼Œç”¨ `"nhql,nlhd->nqhd"` è¡¨ç¤ºäº† $\boldsymbol{A}$ ä¸ $\boldsymbol{V}$ çš„ä¹˜æ³•ï¼Œå…·ä½“æ“ä½œæ˜¯ï¼š

1. å°† $\boldsymbol{V}_{N\times v\times h\times d}$ è½¬ç½®ä¸º $\boldsymbol{V}_{N\times h\times v\times d}$ï¼›
2. å›ºå®šå‰ä¸¤ç»´ï¼Œä»¤ $\boldsymbol{A}_{N\times h\times q\times k}$ ä¸ $\boldsymbol{V}_{N\times h\times v\times d}$ åœ¨åä¸¤ç»´ä¸Šåšä¹˜æ³•ï¼Œè¿™é‡Œæœ‰ $q=k=v$ï¼Œæ‰€ä»¥ç»“æœä¸º $(AV)_{N\times h \times q\times d}$ï¼Œåˆ°è¿™ä¸€æ­¥å·²ç»è®¡ç®—äº† $\mathrm{Softmax}(\boldsymbol{Q}\boldsymbol{K}^\top/\sqrt{d_k})\boldsymbol{V}$ï¼›
3. å°†ç»“æœè½¬ç½®ä¸º $(AV)_{N\times q \times h\times d}$ã€‚

æœ€åä»£ç ä½¿ç”¨ `reshape()` åˆå¹¶åä¸¤ç»´ï¼Œå°†ç»“æœè½¬åŒ–ä¸º $(AV)_{N\times q \times hd}$ï¼Œå¾ˆå·§å¦™åœ°æ‹¼æ¥äº†å¤šä¸ª head çš„æ³¨æ„åŠ›ï¼Œæœ€åé€šè¿‡çº¿æ€§å±‚å†è¾“å‡ºç»“æœã€‚

è‡³æ­¤ï¼ŒTransformer ä¸­çš„ `SelfAttention` éƒ¨åˆ†å·²ç»ç»“æŸï¼Œè¯»è€…æˆ–è®¸ä¼šè§‰å¾—å¤´æ˜è„‘èƒ€ã€‚ä¸å¿…æ‹…å¿ƒï¼Œæœ€ä¸ºè‰°æ¶©çš„ä¸€éƒ¨åˆ†å·²ç»è¿‡å»ï¼Œæ¥ä¸‹æ¥æ˜¯ä¸€è·¯ä¸‹å¡ ğŸš©

### TransformerBlock

```py
class TransformerBlock(nn.Module):
    def __init__(self, embed_size, heads, dropout, forward_expansion):
        super(TransformerBlock, self).__init__()
        # å‰ä¸€å±‚çš„å¤šå¤´æ³¨æ„åŠ›
        self.attention = SelfAttention(embed_size, heads)
        # Add & Norm å±‚
        self.norm1 = nn.LayerNorm(embed_size)
        self.norm2 = nn.LayerNorm(embed_size)
        # å‰é¦ˆå±‚
        self.feed_forward = nn.Sequential(
            nn.Linear(embed_size, forward_expansion * embed_size),
            nn.ReLU(),
            nn.Linear(forward_expansion * embed_size, embed_size),
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, value, key, query, mask):
        attention = self.attention(value, key, query, mask)

        x = self.dropout(self.norm1(attention + query))
        forward = self.feed_forward(x)
        out = self.dropout(self.norm2(forward + x))
        return out
```

`TransformerBlock` æ¨¡å—åŒ…æ‹¬å¤šå¤´æ³¨æ„åŠ›ä¸åæ¥çš„ Add & Normã€Feed Forwardã€Add & Norm ä¸‰å±‚ã€‚

åˆå§‹åŒ–éƒ¨åˆ†ä½¿ç”¨ `nn.Sequential()` å°† `nn.Linear()`ã€`nn.ReLU()`ã€`nn.Linear` ä¾æ¬¡è¿æ¥èµ·æ¥å½¢æˆå‰é¦ˆå±‚ï¼Œæ­£å¦‚å‰æ–‡æ‰€è¯´çš„ï¼Œæ•°æ®è¿›å…¥å‰é¦ˆå±‚å…ˆå‡ç»´å†æ¿€æ´»ï¼Œæœ€åå†é™å›åŸæ¥ç»´åº¦ï¼Œ`forward_expansion` å†³å®šå‡ç»´çš„å€æ•°ã€‚`dropout` ç”¨äºéšæœºå¼ƒç”¨ä¸€éƒ¨åˆ†æ•°æ®é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œç›´æ¥è°ƒç”¨ `nn.Dropout()` ç±»ï¼Œæ¥æ”¶çš„æ•°å€¼å†³å®šäº†å¼ƒç”¨æ•°æ®çš„æ¯”ä¾‹ã€‚

`forward()` éƒ¨åˆ†ä¹Ÿå¾ˆç®€å•ï¼Œè®¡ç®—çš„å¤šå¤´æ³¨æ„åŠ›ä¾æ¬¡åš Add & Normã€Feed Forwardã€Add & Norm ä¸‰å±‚åè¾“å‡ºæ•°æ®ã€‚

### Encoder

```py
class Encoder(nn.Module):
    def __init__(
        self,
        src_vocab_size,
        embed_size,
        num_layers,
        heads,
        device,
        forward_expansion,
        dropout,
        max_length,
    ):

        super(Encoder, self).__init__()
        self.embed_size = embed_size
        # CPU or GPU
        self.device = device
        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)
        self.position_embedding = PositionalEncoding(embed_size, max_length)

        self.layers = nn.ModuleList(
            [
                TransformerBlock(
                    embed_size,
                    heads,
                    dropout=dropout,
                    forward_expansion=forward_expansion,
                )
                for _ in range(num_layers)
            ]
        )

        self.dropout = nn.Dropout(dropout)
```

Encoder æ˜¯ Transformer ä¸­çš„å·¦è¾¹éƒ¨åˆ†ï¼ŒTransformer ä¸­æœ‰ $N$ ä¸ª `TransformerBlock` é¡ºåºå æ”¾åœ¨ä¸€èµ·ç»„æˆ encoderã€‚æ‰€ä»¥åœ¨åˆå§‹åŒ–éƒ¨åˆ†ï¼Œä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼åœ¨ `layers` ä¸­æ”¾ç½®äº† `num_layers` å±‚ `TransformerBlock`ã€‚

```py
# class Encoder(nn.Module):
    def forward(self, x, mask):
        # è¾“å…¥æ•°æ®çš„ batch_size ä¸é•¿åº¦
        N, seq_length = x.shape
        # ä»è¾“å…¥æ•°æ®è®¡ç®—ä½ç½®ç´¢å¼•
        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)
        # ç”±ä½ç½®ç´¢å¼•å¾—åˆ°ä½ç½®ç¼–ç ï¼Œå¹¶ dropout ä¸€éƒ¨åˆ†æ•°æ®
        out = self.dropout(
            (self.word_embedding(x) + self.position_embedding(positions))
        )

        # è®©æ•°æ®é€å±‚ç»è¿‡ encoderï¼Œè®¡ç®—è‡ªæ³¨æ„åŠ›
        for layer in self.layers:
            out = layer(out, out, out, mask)

        return out
```

åœ¨ `forward()` éƒ¨åˆ†ä¸­ï¼Œä½¿ç”¨ `torch.arange()` å¾—åˆ°ä½ç½®ç´¢å¼•ï¼Œå†ç”¨ `expand()` æ–¹æ³•å°†ä½ç½®ç´¢å¼•çŸ©é˜µçš„å½¢çŠ¶å˜ä¸ºä¸è¾“å…¥æ•°æ®ç›¸åŒï¼Œ`expand()` æ–¹æ³•çš„ä¸»è¦ä½œç”¨æ˜¯å¤åˆ¶ï¼Œä¾‹å¦‚ï¼š

```python-repl
>>> torch.arange(0, 5)
tensor([0, 1, 2, 3, 4])
>>> torch.arange(0, 5).expand(2, 5)
tensor([[0, 1, 2, 3, 4],
        [0, 1, 2, 3, 4]])
```

`to()` æ–¹æ³•ç”¨äºæŒ‡å®š Tensor å­˜å‚¨çš„è®¾å¤‡ï¼Œä¾‹å¦‚ `"CPU"` æˆ– `"GPU"`ã€‚å°†è¯åµŒå…¥åŠ ä¸Šä½ç½®ç¼–ç å¾—åˆ° `out`ï¼Œå†å°† `out` é€å…¥ encoder ä¸­è®¡ç®—ç»“æœã€‚

`layer(out, out, out)` çœ‹èµ·æ¥æˆ–è®¸æœ‰äº›å¥‡æ€ªï¼Œè¯·ç•™æ„ï¼Œå‰æ–‡å·²ç»è®¨è®ºè¿‡ï¼Œåœ¨ encoder ä¸­è®¡ç®—çš„æ˜¯**è‡ªæ³¨æ„åŠ›**ï¼Œæ‰€ä»¥æ­¤æ—¶çš„ queryã€keyã€value éƒ½æ˜¯ç›¸åŒçš„ï¼Œè€Œåœ¨ decoder ä¸­å°±ä¼šæœ‰æ‰€ä¸åŒäº†ã€‚

### DecoderBlock

```py
class DecoderBlock(nn.Module):
    def __init__(self, embed_size, heads, forward_expansion, dropout, device):
        super(DecoderBlock, self).__init__()
        self.norm = nn.LayerNorm(embed_size)
        self.attention = SelfAttention(embed_size, heads=heads)
        self.transformer_block = TransformerBlock(
            embed_size, heads, dropout, forward_expansion
        )
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, value, key, src_mask, trg_mask):
        attention = self.attention(x, x, x, trg_mask)
        query = self.dropout(self.norm(attention + x))
        out = self.transformer_block(value, key, query, src_mask)
        return out
```

ç±»ä¼¼åœ°ï¼ŒDecoder æ˜¯ Transformer ç»“æ„å›¾ä¸­çš„å³ä¾§éƒ¨åˆ†ï¼Œä¹Ÿæ˜¯ç”± $N$ å±‚ `DecoderBlock` ç»„æˆã€‚decoder åªæ¯” encoder å¤šäº†ä¸€ä¸ªæ©ç æ³¨æ„åŠ›å±‚ï¼Œå…¶ä»–ç»“æ„ç›¸åŒï¼Œæ‰€ä»¥ `DecoderBlock` çš„åˆå§‹åŒ–ä¸­ç›´æ¥è°ƒç”¨äº†å…ˆå‰å®šä¹‰çš„ `TransformerBlock`ã€‚

`forward()` ä¸­ï¼Œtarget è¿›å…¥ decoder åï¼Œå…ˆè®¡ç®—**è‡ªæ³¨æ„åŠ›**ï¼ˆ`attention(x, x, x)`ï¼‰ï¼Œå†ç»è¿‡ Add & Norm å±‚å¾—åˆ° `query`ï¼Œå†ä¸ encoder ä¸­çš„ç»“æœåšå¤šå¤´æ³¨æ„åŠ›ï¼ˆ`attention(value, key, query)`ï¼‰ï¼Œè¾“å‡ºç»“æœã€‚ç•™æ„ä¸¤ç§æ³¨æ„åŠ›è®¡ç®—çš„ä¸åŒï¼Œå‚è€ƒ Transformer ç»“æ„å›¾ç†è§£ä¸€ä¸‹å°±ä¼šå¾ˆæ˜ç¡®ã€‚

### Decoder

```py
class Decoder(nn.Module):
    def __init__(
        self,
        trg_vocab_size,
        embed_size,
        num_layers,
        heads,
        forward_expansion,
        dropout,
        device,
        max_length,
    ):
        super(Decoder, self).__init__()
        self.device = device
        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)
        self.position_embedding = PositionEmbedding(embed_size,max_length)

        self.layers = nn.ModuleList(
            [
                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)
                for _ in range(num_layers)
            ]
        )
        self.fc_out = nn.Linear(embed_size, trg_vocab_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_out, src_mask, trg_mask):
        N, seq_length = x.shape
        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)
        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))

        for layer in self.layers:
            x = layer(x, enc_out, enc_out, src_mask, trg_mask)

        out = self.fc_out(x)

        return out
```

å®ç°äº† `DecoderBlock` åï¼Œ`Decoder` å°±æ²¡æœ‰ä»€ä¹ˆå†…å®¹äº†ï¼Œä¸ encoder ç±»ä¼¼ï¼Œå°±æ˜¯å°†å¤šä¸ª `DecoderBlock` ç»„è£…èµ·æ¥ï¼ŒæŒ‰æ¥å£ä¼ å…¥æ•°æ®è¿›è¡Œè®¡ç®—ã€‚

### Transformer

æœ€åçš„ `Transformer` å°†å„ä¸ªæ¨¡å—éƒ½ç»„åˆèµ·æ¥ï¼š

```py
class Transformer(nn.Module):
    def __init__(
        self,
        src_vocab_size,
        trg_vocab_size,
        src_pad_idx,
        trg_pad_idx,
        embed_size=512,
        num_layers=6,
        forward_expansion=4,
        heads=8,
        dropout=0,
        device="cpu",
        max_length=100,
    ):

        super(Transformer, self).__init__()

        self.encoder = Encoder(
            src_vocab_size,
            embed_size,
            num_layers,
            heads,
            device,
            forward_expansion,
            dropout,
            max_length,
        )

        self.decoder = Decoder(
            trg_vocab_size,
            embed_size,
            num_layers,
            heads,
            forward_expansion,
            dropout,
            device,
            max_length,
        )

        self.src_pad_idx = src_pad_idx
        self.trg_pad_idx = trg_pad_idx
        self.device = device

    def make_src_mask(self, src):
        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)
        # (N, 1, 1, src_len)
        return src_mask.to(self.device)

    def make_trg_mask(self, trg):
        N, trg_len = trg.shape
        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(
            N, 1, trg_len, trg_len
        )

        return trg_mask.to(self.device)

    def forward(self, src, trg):
        src_mask = self.make_src_mask(src)
        trg_mask = self.make_trg_mask(trg)
        enc_src = self.encoder(src, src_mask)
        out = self.decoder(trg, enc_src, src_mask, trg_mask)
        return out
```

åˆå¦‚åŒ–éƒ¨åˆ†ä¸»è¦æ˜¯è®¾å®šäº†é»˜è®¤çš„å‚æ•°ï¼Œå¹¶å¼•å…¥å‰é¢å®šä¹‰å¥½çš„ `Encoder` ä¸ `Decoder` æ¨¡å—ã€‚`Transformer` ä¸­è¿˜å¤šäº† `make_src_mask()` ä¸ `make_trg_mask()` ä¸¤ä¸ªå‡½æ•°ï¼Œè¿™å°±ä¸å¾—ä¸è°ˆè°ˆ Transformer ä¸­çš„æ©ç æœºåˆ¶äº†ã€‚

è€ƒè™‘ä¸€ä¸ªæƒ…å¢ƒï¼Œéœ€è¦ä½¿ç”¨ Transformer ç¿»è¯‘ä¸€æ‰¹ï¼ˆè‹¥å¹²æ¡ï¼‰å¥å­ï¼Œå„å¥å­çš„é•¿åº¦è‡ªç„¶æ˜¯ä¸åŒçš„ï¼Œé‚£ä¹ˆè¾“å…¥æ¨¡å‹çš„æ•°æ®çš„å½¢çŠ¶ä¹Ÿæ˜¯ä¸åŒçš„ï¼Œè¿™åœ¨åç»­æ­¥éª¤ä¸­å°±ä¼šå‡ºç°å¾ˆå¤šé—®é¢˜ã€‚åœ¨å®é™…ä¸­ï¼Œé€šå¸¸ä¼šæ‰¾åˆ°æ–‡æœ¬ä¸­æœ€é•¿çš„å¥å­ï¼ˆ`max_len`ï¼‰ï¼Œå†å°†æ‰€æœ‰å¥å­éƒ½å˜ä¸ºè¯¥é•¿åº¦ï¼Œè¿™ç§æ“ä½œç§°ä¸º paddingã€‚

å…·ä½“åšæ³•å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåˆ†åˆ«ç”¨ `<s>` ä¸ `<e>` æ ‡è®°å¥å­çš„èµ·è®«ï¼Œç”¨ `<p>` å¡«å…… `<e>` åçš„ç©ºä½ï¼Œå„æ•°æ®çš„é•¿åº¦å°±ä¼šä¸€è‡´ã€‚ç„¶åæ ¹æ®è®¾å®šçš„è¯å…¸ï¼Œå°† token è½¬åŒ–ä¸ºç´¢å¼•ï¼Œæ¥ç€å†åšè¯åµŒå…¥ã€‚`make_src_mask()` å°±æ˜¯æ ¹æ® `<p>` çš„ç´¢å¼•ï¼Œå°† `<p>` æ‰€åœ¨ä½ç½®éƒ½æ ‡è®°ä¸º `False`ï¼Œå…¶ä»–ä½ç½®æ ‡è®°ä¸º `True`ã€‚

åç»­ `unsqueeze()` çš„æ“ä½œæ¯”è¾ƒè´¹è§£ï¼Œå…¶å®å®ƒæ˜¯åˆ©ç”¨äº† PyTorch çš„å¹¿æ’­æœºåˆ¶ï¼Œç”¨äºè‡ªåŠ¨åŒ¹é…çŸ©é˜µçš„å½¢çŠ¶ã€‚å›¾ä¸­çš„ä¾‹å­å¯ä»¥çœ‹ä½œæ˜¯å°†çŸ©é˜µç¿»è½¬å†åœ¨ç¬¬ 3 ä¸ªæ–¹å‘ä¸Šæ‹‰é•¿ã€‚å› ä¸ºä»£ç ä¸­çš„æ©ç è¦ç”¨äºæ©ç›–å½¢çŠ¶ä¸º `[N, heads, query_len, key_len]` å…·æœ‰ 4 ä¸ªæ–¹å‘çš„ `energy`ï¼Œæ‰€ä»¥è¦é¢å¤–å†åšä¸€æ¬¡ `unsqueeze()`ã€‚æœ€åå°†æ©ç ç”¨äºæ©ç›–è¯åµŒå…¥æ•°æ®ï¼Œæ©ç å°±åƒä¸€ä¸ªç½©å­ç›–åœ¨è¯åµŒå…¥æ•°æ®ä¸Šï¼Œæ¨¡å‹åªè®¡ç®— `True` ä½ç½®ä¸Šçš„æ•°æ®ã€‚

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8821?authkey=ALYpzW-ZQ_VBXTU)

ä½¿ç”¨æ©ç å¯ä»¥è®©æ¨¡å‹çµæ´»åœ°å¤„ç†ä¸åŒé•¿åº¦çš„æ•°æ®ï¼Œæ•°æ®çš„é•¿åº¦ç”±æ©ç å†³å®šï¼Œæ”¹å˜æ©ç å°±ç›¸å½“äºæ”¹å˜å¤„ç†çš„æ•°æ®ï¼Œè€Œä¸å»æ”¹å˜å­˜å‚¨åœ¨ç¡¬ä»¶ä¸­çš„æ•°æ®ï¼Œè¿™å¯¹äºè®¡ç®—æ›´æœ‰åˆ©ã€‚

`make_trg_mask()` å‡½æ•°äº§ç”Ÿç”¨äº target æ•°æ®çš„æ©ç ï¼Œåœ¨ target ä¸Šä½¿ç”¨æ©ç çš„åŸå› ä¸ source ä¸åŒã€‚åœ¨ decoder ä¸­ï¼Œæ¨¡å‹è¦æ ¹æ®è¾“å…¥æ•°æ®çš„è®¡ç®—ç»“æœç»™å‡ºæ–° tokenï¼Œè€Œç”Ÿæˆæ–‡æœ¬çš„è¿‡ç¨‹æ˜¯é¡ºåºçš„ï¼Œä¾èµ–äºå‰ä¸€æ­¥ç”Ÿæˆçš„ç»“æœã€‚å…·ä½“æ¥è¯´å°±æ˜¯ï¼Œ

1. åºåˆ—ä»¥ `<s>` æ ‡è®°èµ·å§‹ï¼›
2. æ ¹æ®å·²æœ‰çš„ `<s>` ç”Ÿæˆ `A`ï¼›
3. æ ¹æ®ç”Ÿæˆçš„ `<s> A` ç”Ÿæˆ `B`ï¼›
4. æ ¹æ®ç”Ÿæˆçš„ `<s> A B` ç”Ÿæˆ `C`ï¼›
5. ä»¥æ­¤ç±»æ¨ï¼Œç›´è‡³æ¨¡å‹ç”Ÿæˆ `<e>`ï¼Œå¥å­ç»“æŸã€‚

å‰æ–‡å·²ç»è®¨è®ºè¿‡ï¼Œè¿™ç§æ–¹æ³•æœ‰å¾ˆå¤šå±€é™æ€§ï¼Œè€Œ Transformer çš„å·§å¦™ä¹‹å¤„å°±åœ¨äºèƒ½å¤Ÿå¹¶è¡Œå®Œæˆè¿™ä¸ªè¿‡ç¨‹ã€‚

æˆ‘ä»¬å¯ä»¥è€ƒè™‘è®­ç»ƒè¿‡ç¨‹ï¼Œå®é™…ä¸Šä¸ç”Ÿæˆè¿‡ç¨‹ç±»ä¼¼ï¼Œè®­ç»ƒè¿‡ç¨‹å°±æ˜¯è¦æ ¹æ®å·²ç»ç”Ÿæˆçš„ `<s>` å»ºç«‹ä¸ä¸‹ä¸€ä¸ª token `A` çš„å…³ç³»ï¼Œè€Œä¸èƒ½æ˜¯ä¸åç»­ `B` æˆ– `C` çš„å…³ç³»ï¼Œå°†è¿™ç§å…³ç³»ä»¥å‚æ•°çš„å½¢å¼å­˜å‚¨åˆ°æ¨¡å‹ä¸­ï¼Œæ¨ç†é˜¶æ®µå°±èƒ½é¡ºåˆ©åœ°æ ¹æ® `<s>` ç”Ÿæˆ `A`ã€‚è¿™æ ·çš„è®­ç»ƒè¿‡ç¨‹å¯ä»¥è¡¨ç¤ºä¸ºä¸€ä¸ªä¸‹ä¸‰è§’çŸ©é˜µï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8822?authkey=ALYpzW-ZQ_VBXTU)

Transformer ä¸éœ€è¦é€ä¸ª token ç”Ÿæˆå†å»ºç«‹å…³ç³»ï¼Œå¯ä»¥é€šè¿‡ä¸‹ä¸‰è§’çŸ©é˜µä¸€æ¬¡ç›´æ¥å–å‡º `<s>`ã€`<s> A`ã€`<s> A B` ç­‰ token åºåˆ—ï¼Œå¹¶è¡Œåœ°è®­ç»ƒæ¨¡å‹ä¸å¯¹åº”çš„ä¸‹ä¸€ä¸ª token å»ºç«‹å…³ç³»ã€‚æœ€åå°† `<s>` ä¸æ¯ä¸€æ­¥éª¤ä¸­æ–°ç”Ÿæˆ token `A`ã€`B`ã€`C`ã€`<e>` æ‹¼åˆèµ·æ¥ï¼Œå³å¾—åˆ°ç”Ÿæˆçš„æ–‡æœ¬ã€‚

`make_trg_mask()` å°±æ˜¯åœ¨æ„å»ºè¿™ä¸ªä¸‹ä¸‰è§’çš„æ©ç ã€‚`torch.ones()` ç”¨äºç”ŸæˆæŒ‡å®šå¤§å°å…ƒç´ å…¨ä¸º `1` çš„çŸ©é˜µï¼Œç„¶åç”¨ `torch.tril()` å–è¯¥çŸ©é˜µçš„ä¸‹ä¸‰è§’ï¼Œå†ç”¨ `expand()` æ–¹æ³•å°†è¯¥çŸ©é˜µå¤åˆ¶åˆ°ä¸ `batch_size` åŒ¹é…ã€‚

### Train

ä»å‰é¢è®¨è®ºçš„æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹è¿˜å¯ä»¥çŸ¥é“çš„ä¸€ç‚¹æ˜¯ï¼Œæ¨¡å‹æ°¸è¿œä¸ä¼šç”Ÿæˆ `<s>`ï¼Œæ‰€ä»¥ target ä¸­æ²¡æœ‰ `<s>`ï¼Œè€Œ source åˆ™å¿…é¡»ç”± `<s>` èµ·å§‹ã€‚åœ¨å®é™…ä¸­ï¼Œä¸€ç§åšæ³•æ˜¯ï¼Œç”¨é¢„å¤„ç†çš„è„šæœ¬åœ¨åŸå§‹è®­ç»ƒæ•°æ®ï¼ˆä¾‹å¦‚ `.csv`ã€`.txt` æ–‡ä»¶ï¼‰ä¸­æ ‡ä¸Šæ ‡è®°ï¼›å¦ä¸€ç§æ–¹æ³•æ˜¯ï¼Œåœ¨è®­ç»ƒä»£ç ä¸­åŠ å…¥é¢„å¤„ç†çš„åŠŸèƒ½ï¼Œè¯»å–æ•°æ®æ—¶åˆ†åˆ«ä¸ºæ•°æ®åšä¸Šç›¸åº”æ ‡è®°ã€‚ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæœ¬æ–‡å°±ä¸å®ç°è¿™ä¸€éƒ¨åˆ†åŠŸèƒ½ï¼Œä½¿ç”¨ Transformer å¯ä»¥ç›´æ¥å¤„ç†çš„æ•°æ®ã€‚

ç”Ÿæˆè®­ç»ƒæ•°æ®çš„å‡½æ•°ä¸º

```py
def generate_random_batch(batch_size, max_length=16):
    src = []
    for i in range(batch_size):
        # éšæœºæŒ‡å®šæœ‰æ•ˆæ•°æ®çš„é•¿åº¦
        random_len = random.randint(1, max_length - 2)
        # åœ¨æ•°æ®èµ·è®«å¤„åŠ ä¸Šæ ‡è®°ï¼Œ"<s>": 0, "<e>": 1
        random_nums = [0] + [random.randint(3, 9) for _ in range(random_len)] + [1]
        # padding å¡«æ»¡æ•°æ®é•¿åº¦ï¼Œ"<p>": [2]
        random_nums = random_nums + [2] * (max_length - random_len - 2)
        src.append(random_nums)

    src = torch.LongTensor(src)
    # tgt å»é™¤æœ«å°¾çš„ token
    tgt = src[:, :-1]
    # tgt_y å»é™¤é¦–ä¸ª <s>ï¼Œå³æ¨¡å‹éœ€è¦é¢„æµ‹çš„ tokenï¼Œç”¨äºè®¡ç®—æŸå¤±
    tgt_y = src[:, 1:]
    # æ¨¡å‹éœ€è¦é¢„æµ‹çš„ token æ•°é‡ï¼ˆä¸è®¡ <p>ï¼‰ï¼Œç”¨äºè®¡ç®—æŸå¤±å‡½æ•°
    n_tokens = (tgt_y != 2).sum()

    return src, tgt, tgt_y, n_tokens
```

`generate_random_batch()` èƒ½å¤Ÿç”Ÿæˆ Transformer å¯ä»¥ç›´æ¥è®¡ç®—çš„ç›¸åŒçš„ source ä¸ targetï¼Œè¯¥æ¨¡å‹çš„ä»»åŠ¡ç›®æ ‡å°±æ˜¯ç”Ÿæˆä¸è¾“å…¥ç›¸åŒçš„åºåˆ—ã€‚æ¨¡å‹ä¸ä¼šç”Ÿæˆ `<s>`ï¼Œæ‰€ä»¥`tgt_y` å»é™¤ `<s>` ç”¨äºä¸ç”Ÿæˆçš„åºåˆ—å¯¹æ¯”è®¡ç®—æŸå¤±ï¼Œè¿™å¾ˆå®¹æ˜“ç†è§£ã€‚ä½†ä¸ºä»€ä¹ˆ `tgt` éœ€è¦å»é™¤æœ€åä¸€ä¸ª token å‘¢ï¼Ÿè¿™ä¸€ç‚¹æˆ‘å°†åœ¨åæ–‡ç”Ÿæˆåºåˆ—çš„ Predict ä¸€èŠ‚è®¨è®ºã€‚è®­ç»ƒä¸æµ‹è¯•æ¨¡å‹çš„ä»£ç å¦‚ä¸‹ï¼š

```py
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

# <p> ç´¢å¼•
src_pad_idx = 2
trg_pad_idx = 2
# è¯è¡¨å¤§å°ï¼Œå³å…¨éƒ¨ token æ•°é‡ï¼ŒåŒ…æ‹¬ <s> <e> <p> ç­‰æ ‡è®°
src_vocab_size = 10
trg_vocab_size = 10
# æ–‡æœ¬æœ€å¤§é•¿åº¦
max_len = 16

model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx,
                    embed_size=128, num_layers=2, dropout=0.1, max_length=max_len,
                    device=device).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)
criteria = nn.CrossEntropyLoss()
total_loss = 0

for step in range(2000):
    src, tgt, tgt_y, n_tokens = generate_random_batch(batch_size=2, max_length=max_len)
    optimizer.zero_grad()
    out = model(src, tgt)

    # contiguous() ä¸ view() å°†çŸ©é˜µåœ¨å„è¡Œé¦–å°¾ç›¸è¿ä¸ºä¸€è¡Œï¼ˆå³å‘é‡ï¼‰
    # åœ¨ä¸¤å‘é‡é—´è®¡ç®—æŸå¤±å‡½æ•°
    # tgt_y ä¸­å…ƒç´ çš„å€¼æ˜¯ç´¢å¼•ï¼Œé™¤ä»¥ n_tokens å°†å…¶ç¼©æ”¾åˆ° [0, 1]
    loss = criteria(out.contiguous().view(-1, out.size(-1)),
                    tgt_y.contiguous().view(-1)) / n_tokens
    loss.backward()
    optimizer.step()

    total_loss += loss

    if step != 0 and step % 40 == 0:
        print(f"Step {step}, total_loss: {total_loss}")
        total_loss = 0

# Predict
copy_test(model, max_len)
```

PyTorch ä½¿ç”¨ `torch.optim` å®šä¹‰æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå…¶ä¸­å¯ä»¥é€‰æ‹©éå¸¸å¤šç§çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œè¿™é‡Œé€‰æ‹©äº† `Adam()`ï¼Œ`lr=3e-4` æŒ‡å®šäº†è®­ç»ƒæ­¥éª¤çš„å­¦ä¹ ç‡ã€‚`nn.CrossEntropyLoss()` ç”¨äºè®¡ç®—ä¸¤ä¸ªå‘é‡çš„äº¤å‰ç†µæŸå¤±ï¼Œä½œä¸ºè®­ç»ƒè¿‡ç¨‹çš„æŸå¤±å‡½æ•°ã€‚

åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼Œæ¯ä¸€ä¸ªå¾ªç¯å¤„ç† 1 ä¸ª batch çš„æ•°æ®ï¼Œåœ¨åŒä¸€ä¸ª batch ä¸­ PyTorch è‡ªåŠ¨è®¡ç®—æ¢¯åº¦çš„åå‘ä¼ æ’­å¹¶æ›´æ–°å‚æ•°ã€‚ä½†åœ¨æ–°çš„ batch ä¸­ï¼Œå› ä¸ºå·²ç»æ›´æ–°åˆ°å‚æ•°ä¸­äº†ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›ä¿ç•™ä¸Šä¸€ä¸ª batch çš„æ¢¯åº¦ï¼Œæ‰€ä»¥ç”¨ `optimizer.zero_grad()` å°†æ¢¯åº¦æ¸…ç©ºã€‚

å°† `src` ä¸ `tgt` ä¼ å…¥æ¨¡å‹ï¼Œ`out` å°±æ˜¯ Transformer çš„è®¡ç®—ç»“æœã€‚`loss.backward()` ä¸ `optimizer.step()` ä¸¤è¡Œä»£ç å°±æ˜¯å‰é¢æ‰€è¯´çš„è®© PyTorch è‡ªåŠ¨è®¡ç®—æ¢¯åº¦çš„åå‘ä¼ æ’­å¹¶æ›´æ–°å‚æ•°ã€‚

### Predict

è®­ç»ƒç»“æŸåï¼Œæˆ‘ç”¨ `copy_test()` å‡½æ•°æµ‹è¯•æ¨¡å‹çš„æ•ˆæœï¼Œè¿™ä¸ªæµ‹è¯•å‡½æ•°å®šä¹‰ä¸º

```py
def copy_test(model, max_len):
    model = model.eval()
    src = torch.LongTensor([[0, 6, 3, 4, 5, 6, 7, 4, 3, 1, 2, 2]])
    # æ¨¡å‹ä» <s> å¼€å§‹ç”Ÿæˆåºåˆ—ï¼Œä½†ä¸ä¼šç”Ÿæˆ <s>ï¼Œæ‰€ä»¥æŒ‡å®šèµ·å§‹çš„ <s>
    tgt = torch.LongTensor([[0]])

    for i in range(max_len):
        # outï¼š (1, i + 1, 10)
        # i + 1 æ¨¡å‹è¾“å‡ºçš„ token æ•°é‡
        # 10 ä¸º vocab_sizeï¼Œæ˜¯è¯è¡¨ä¸­ token æ•°é‡ï¼Œout æ˜¯è¯è¡¨ä¸­å„ token åœ¨æ­¤å¤„å‡ºç°çš„æ¦‚ç‡
        out = model(src, tgt)
        # å–è¾“å‡ºçš„ i + 1 ä¸ª token ä¸­çš„æœ€åä¸€ä¸ª
        # predict: (1, 10)
        predict = out[:, -1]
        # å–å¾—æ¦‚ç‡æœ€å¤§çš„ token ç´¢å¼•
        # y: (1, )
        y = torch.argmax(predict, dim=1)
        # é€ä¸ªæ‹¼åˆ token ç´¢å¼•
        # y.unsqueeze(0): (1, 1)
        # tgt: (1, i + 1 )
        tgt = torch.concat([tgt, y.unsqueeze(0)], dim=1)
        # è‹¥ç”Ÿæˆ token <e>ï¼Œè¡¨ç¤ºå¥å­ç»“æŸï¼Œé€€å‡ºå¾ªç¯
        if y == 1:
            break
    print(tgt)
```

`eval()` æ–¹æ³•ä»¤æ¨¡å‹é€€å‡ºè®­ç»ƒæ¨¡å¼ï¼Œä¼šå…³é—­ dropout ç­‰è®­ç»ƒè¿‡ç¨‹ä¸­æ‰éœ€è¦çš„åŠŸèƒ½ã€‚åœ¨å¾ªç¯ä¸­é€ä¸ªæ‹¼åˆç”Ÿæˆçš„ tokenï¼Œå°±èƒ½å¾—åˆ°ç”Ÿæˆçš„å¥å­ã€‚å¾ªç¯ä¸­çš„æ“ä½œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåœ¨ç¬¬ 1 æ¬¡å¾ªç¯ä¸­ï¼Œ`tgt` ä¸º `<s>`ï¼Œé€šè¿‡ä¸ `src` çš„æ³¨æ„åŠ›ä¸ä¸‹ä¸‰è§’çŸ©é˜µå¾—åˆ°è®¡ç®—ç»“æœ `out` ä¸º `A`ï¼Œç„¶åå°† `tgt` æ›´æ–°ä¸º `<s> A`ï¼Œåœ¨ç¬¬ 2 æ¬¡å¾ªç¯ä¸­ï¼Œå¾—åˆ°çš„ `out` ä¸º `A B`ï¼Œæ‰€ä»¥åœ¨æ¯æ¬¡å¾ªç¯ä¸­éƒ½åªå–æ–°ç”Ÿæˆçš„ `out[-1]` æ›´æ–° `tgt`ï¼Œæœ€åå°†ç»“æœæ‹¼æ¥èµ·æ¥å¾—åˆ°å®Œæ•´çš„è¾“å‡ºç»“æœã€‚

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8824?authkey=ALYpzW-ZQ_VBXTU)

æˆ–è®¸è¯»è€…ä¼šæœ‰ç–‘æƒ‘ï¼Œæ—¢ç„¶ä½¿ç”¨ä¸‹ä¸‰è§’çŸ©é˜µå¹¶è¡Œè®¡ç®—æ˜¯ Transformer çš„ä¼˜åŠ¿ï¼Œä¸ºä»€ä¹ˆè¿™é‡Œå´æ˜¯ç”¨å¾ªç¯é¡ºåºåœ°ç”Ÿæˆå‘¢ï¼Ÿä¸ºä»€ä¹ˆè®¡ç®—ä¸Šå›¾ä¸­æœ€åä¸€ä¸ªçŸ©é˜µçš„ `out`ï¼Œè€Œæ˜¯è¦ç”¨ä¸€ä¸ªä¸ªçš„ `out[-1]` å‘¢ï¼Ÿ

è¦æ³¨æ„çš„æ˜¯ï¼Œè®­ç»ƒä¸ç”Ÿæˆæœ‰é‡è¦çš„ä¸€ä¸ªä¸åŒï¼Œå°±æ˜¯ç”Ÿæˆä¸­çš„ `tgt` æ˜¯ç©ºç™½çš„ã€æ¨¡å‹ä¸å¯çŸ¥çš„ï¼Œè€Œè®­ç»ƒä¸­çš„ `tgt` æ˜¯å®Œæ•´çš„ã€æ¨¡å‹å¯çŸ¥çš„ã€‚å¦‚ä¸Šå›¾ä¸­ï¼Œ`tgt` åœ¨æ¯ä¸ªå¾ªç¯ä¸­éƒ½åœ¨å˜é•¿ï¼Œåªæœ‰ `tgt` å˜æˆäº† `<s> A B C â€¦` æ‰ä¼šæœ‰æœ€åä¸€ä¸ªçŸ©é˜µä¸­çš„ `out`ã€‚å¦‚æœè¯´åªè¦æœ€åä¸€ä¸ªçŸ©é˜µä¸­çš„ `out` è€Œä¸è¦å‰é¢çš„æ­¥éª¤ï¼Œå°±å˜æˆäº†ã€Œåƒä¸¤ä¸ªé¦’å¤´åƒé¥±ï¼Œæ‰€ä»¥åªåƒåä¸€ä¸ªèƒ½åƒå¾—é¥±çš„é¦’å¤´ã€çš„ç¬‘è¯ã€‚

æ‰€ä»¥<dot>ç”Ÿæˆè¿‡ç¨‹å¹¶ä¸æ˜¯å¹¶è¡Œçš„ï¼ŒTransformer çš„å¹¶è¡ŒæŒ‡çš„æ˜¯è®­ç»ƒè¿‡ç¨‹</dot>ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ Transformer åªéœ€è¦åšä¸€æ¬¡ä¸‹ä¸‰è§’çŸ©é˜µçš„è¿ç®—å°±å¯ä»¥å»ºç«‹å¤šä¸ª token é—´çš„å…³ç³»ã€‚è¿™å¼ å›¾è¿˜è§£é‡Šäº†æ¨¡å‹æ°¸è¿œä¸ä¼šç”Ÿæˆ `<s>` ä½† `tgt` å¿…é¡»ä»¥ `<s>` èµ·å§‹çš„åŸå› ã€‚å›¾ä¸­è¿˜å¯ä»¥å¾ˆæ˜ç™½çš„çœ‹å‡ºä¸ºä»€ä¹ˆå…ˆå‰çš„è®­ç»ƒä»£ç è¦å»é™¤ `tgt` æœ«å°¾çš„ tokenï¼Œå› ä¸º Transformer çš„è¾“å‡º `out` è®¡ç®—çš„æ˜¯ `tgt` ä¸‹ä¸€ä¸ª tokenï¼ˆåŠæ­¤å‰ï¼‰çš„è®¡ç®—ç»“æœï¼Œè‹¥ä¸å»é™¤æœ«ä½å°±è¶…å‡ºèŒƒå›´äº†ã€‚

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8823?authkey=ALYpzW-ZQ_VBXTU)

æœ€åè®­ç»ƒä¸æµ‹è¯•çš„ç»“æœä¸º

```python-repl
cpu
Step 40, total_loss: 4.021485328674316
Step 80, total_loss: 2.8817126750946045
â€¦â€¦
Step 1920, total_loss: 0.9760974049568176
Step 1960, total_loss: 0.8644390106201172
tensor([[0, 6, 3, 4, 5, 7, 6, 4, 3, 1]])
```

è¾“å‡ºçš„ç»“æœæ²¡æœ‰è¾“å‡º source `[[0, 6, 3, 4, 5, 6, 7, 4, 3, 1, 2, 2]]` ä¸­æœ«å°¾ä»£è¡¨ `<p>` çš„ `2`ï¼Œå‰é¢çš„ token ç´¢å¼•ä¹Ÿä¸ source ç›¸å·®æ— å‡ ï¼Œè¯´æ˜æ¨¡å‹æ­£ç¡®å¤åˆ¶äº†è¾“å…¥åºåˆ—ï¼Œè®­ç»ƒæ˜¯æˆåŠŸçš„ã€‚

## åè®°

è‡³æ­¤ï¼Œè¿™ç¯‡ Transformer çš„ä»‹ç»ç»ˆäºå‘Šä¸€æ®µè½äº†ã€‚ä»èµ·è‰ã€ç»˜å›¾å†åˆ°æœ€åçš„ä»£ç æ¢³ç†ï¼Œå‰åèŠ±äº†ä¸€å‘¨å¤šçš„æ—¶é—´ã€‚è™½åä¸ºä»‹ç»ï¼Œå…¶å®è¿˜æ˜¯ä¸ºè‡ªå·±åœ¨åšæ¢³ç†ï¼Œè¾¹å†™è¾¹æƒ³ã€è¾¹æƒ³è¾¹æŸ¥ï¼Œç»ˆäºæŠŠ Transformer ä¸­çš„ä¸€äº›ç»†èŠ‚å¼„æ˜ç™½äº†ï¼Œè¿™ç¯‡ç¬”è®°ä¹Ÿèƒ½ä¸ºè¯»è€…å‹¾å‹’å‡ºä¸€ä¸ªå¤§è‡´çš„å›¾æ™¯ã€‚

å½“ç„¶ï¼Œé™äºç¯‡å¹…ï¼Œé™äºã€Œä»é›¶èµ·æ­¥ã€çš„åˆè¡·ï¼Œä¹Ÿé™äºç¬”åŠ›ï¼Œè¿˜æœ‰è®¸å¤šæ›´æ·±å±‚æ¬¡é—®é¢˜éƒ½æ²¡æœ‰æ¢è®¨ï¼Œä½†æˆ‘ç›¸ä¿¡ï¼Œåœ¨çœ‹æ‡‚äº†è¿™ç¯‡ç¬”è®°ä¹‹åï¼Œå†å»é˜…è¯»é‚£äº›æ–‡ç« å·²ç»ä¸æˆé—®é¢˜äº†ï¼Œè¿™ä¹Ÿç¬¦åˆæˆ‘çš„åˆå¿ƒã€‚

æˆ–è®¸è¯»è€…è¿˜å¾ˆå›°æƒ‘ï¼Œç–‘æƒ‘ä¸ºä»€ä¹ˆæ•°å­¦æ¨å¯¼ä¸Šå¹¶ä¸é‚£ä¹ˆä¸¥è°¨çš„æ¨¡å‹å±…ç„¶èƒ½æœ‰æ•ˆï¼Œç”šè‡³å…·æœ‰æå¥½çš„è¡¨ç°ï¼Œé‚£å°±è¯´æ˜éœ€è¦é’»å…¥ç ”ç©¶ Transformer çš„åº•å±‚äº†ï¼Œä¸å¯ä¸å†è¯»äº›æ›´ä¸“ä¸šçš„æ–‡ç« ã€‚æˆ‘ä¹ŸæŠŠå†™è¿™ç¯‡æ–‡ç« æ—¶æ‰€å‚è€ƒä»¥åŠè¾ƒå¥½çš„ç›¸å…³èµ„æ–™ç½—åˆ—äºåï¼Œä»¥é£¨è¯»è€…ã€‚

## References

- [Vaswani, A. et al. Attention Is All You Need (2017) - arXiv](https://arxiv.org/abs/1706.03762)
- [ã€ŠAttention is All You Needã€‹æµ…è¯»ï¼ˆç®€ä»‹+ä»£ç ï¼‰- ç§‘å­¦ç©ºé—´](https://spaces.ac.cn/archives/4765)
- [ä»è¯­è¨€æ¨¡å‹åˆ° Seq2Seqï¼šTransformer å¦‚æˆï¼Œå…¨é  Mask - ç§‘å­¦ç©ºé—´](https://spaces.ac.cn/archives/6933)
- [Language Modeling with nn.Transformer and torchtext - PyTorch](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
- [The Illustrated Transformer - Jay Alammar](http://jalammar.github.io/illustrated-transformer/)
- [Transformer æºç ä¸­ Mask æœºåˆ¶çš„å®ç° - åšå®¢å›­](https://www.cnblogs.com/wevolf/p/12484972.html)
- [torch.einsum è¯¦è§£ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/434232512)
- [Pytorch ä¸­ nn.Transformer çš„ä½¿ç”¨è¯¦è§£ä¸ Transformer çš„é»‘ç›’è®²è§£ - CSDN åšå®¢](https://blog.csdn.net/zhaohongfei_358/article/details/126019181)