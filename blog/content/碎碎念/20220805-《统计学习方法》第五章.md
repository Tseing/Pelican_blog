title: 《统计学习方法》第五章：决策树
slug:  statistical-learning-chapter5
date: 2022-08-05
tags: 统计学习方法, Machine learning, Algorithm
summary: 
status: hidden

## 模型

决策树是一种<em>分类</em>与<em>回归</em>的方法，它是由结点和有向边组成的树形结构。决策树的内部结点表示一个特征或属性，叶结点表示一个类。

![决策树模型](https://storage.live.com/items/4D18B16B8E0B1EDB!7527?authkey=ALYpzW-ZQ_VBXTU)

决策树学习的目标就是根据给定的训练集生成一个决策树模型，利用该模型对实例正确分类。也就是说决策树的生成在本质上是从训练集中归纳出一组分类规则，决策树中结点之间的路径正是对应了这种分类规则。

## 策略

决策树可以看作为一系列 if-then 规则的集合，那么如何选择规则的判断条件就十分重要了。选择判断条件就是在选择训练数据的特征，可以想象，如果利用一个特征作为分类条件得到的结果与<em>随机分类</em>的结果没有很大差别，那么这个特征是没有分类能力的。为了生成精确的决策树模型，我们需要设立用于衡量所选特征分类能力的准则。

常用选择特征的准则包括信息增益、信息增益比、平方误差和基尼指数。

### 信息增益

设一个离散随机分布为

$$P(X=x_i)=p_i,\qquad i=1,2,\cdots,n$$

那么随机变量 $X$ 的熵就定义为

$$H(p)=-\sum_{i=1}^np_i\log p_i$$

若 $p_i=0$，规定 $0\log 0=0$，其中的对数也可取 2 为底或取自然对数。熵是衡量随机变量不确定（混乱）程度的度量，熵值越大，则随机变量越不确定，也就是说各事件发生的概率值越接近，概率分布也更分散。

![熵与概率的关系](https://storage.live.com/items/4D18B16B8E0B1EDB!7526?authkey=ALYpzW-ZQ_VBXTU)

条件熵用于表示在某条件下随机变量的不确定性，条件熵具有条件期望的形式，定义为

$$H(X|Y)=\sum_{i=1}^np_iH(Y|X=x_i)$$

在实际操作中会使用频率估计概率，这时候熵就称为经验熵，条件熵称为经验条件熵。

特征 $A$ 对训练集 $D$ 的信息增益就定义为训练集的经验熵与给定特征下的经验条件熵之差：

$$g(D,A)=H(D)-H(D|A)$$

可以直观地理解为，若特征 $A$ 具有分类能力，在选定的特征 $A$ 下，数据集表现出更大的确定性或有序性，熵值相应减小，那么熵值变化的大小就可以用来衡量数据集在应用该特征后确定性的增加程度，也就是该特征分类能力的强弱。因此，信息增益大的特征具有更强的分类能力。

对于训练集 $D$，用 $|D|$ 表示集合中元素个数，即样本个数。设有 $K$ 个类 $C_k$，类 $C_k$ 中有 $|C_k|$ 个样本。依据特征 $A$ 可将 $D$ 划分为 $n$ 个子集 $D_i$，$D_i$ 中属于类 $C_k$ 的样本的集合记作 $D_{ik}$。那么就按以下方法计算信息增益。

**算法 5.1**

> 输入：训练集 $D$ 和特征 $A$  
> 输出：特征 $A$ 对训练集 $D$ 的信息增益

1. 计算数据集 $D$ 的经验熵
    $$H(D)=-\sum_{k=1}^Kp_k\log_2p_k=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}$$
2. 计算特征 $A$ c对数据集的经验条件熵
    $$H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}$$
3. 计算信息增益
    $$g(D,A)=H(D)-H(D|A)$$

### 信息增益比

若参考信息增益比的大小选择特征，选择结果会偏向取值较多的特征。可以这么理解，特征取值越多，即划分条件越多，例如按年龄将人群分为青年、中年、老年（特征取值为 3）相较于按出生地将人群划分为是否当地居民（特征取值为 2），得到的划分结果会更「有序」。但这并不意味着年龄这一特征具有更好的分类能力，分类结果可能与被调查人的出生地有着更为密切的关系。

当特征取值过多时，这个特征就像是精心制作的筛子，过筛后的结果总是「显得」更加有序，因此需要使用信息增益比校正这一问题。信息增益比定义为

$$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$$

$$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}$$

其中 $n$ 为特征 $A$ 的取值个数。

### 平方误差

### 基尼指数

基尼指数也是特征分类能力的度量，常用于分类树。