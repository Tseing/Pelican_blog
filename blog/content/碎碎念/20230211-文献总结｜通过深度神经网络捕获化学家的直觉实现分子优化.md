title: 文献总结｜通过深度神经网络捕获化学家的直觉实现分子优化
slug: summary-doi.org/10.1186/s13321-021-00497-0
date: 2023-02-11
tags: Literature Summary, CADD, Transformer, RNN
summary: 本文介绍于 2021 年发表在 *Journal of Cheminformatics* 上的一篇文章，文章原标题为 Molecular optimization by capturing chemist’s intuition using deep neural networks，文章使用通过 MMP 算法生成的分子对数据和分子性质数据训练了 Transformer 和 seq2seq 等模型，这些模型能够通过结构改造得到具有目标性质的分子。

<i class="fa fa-external-link"></i> [doi.org/10.1186/s13321-021-00497-0](https://doi.org/10.1186/s13321-021-00497-0)

本文介绍于 2021 年发表在 *Journal of Cheminformatics* 上的一篇文章，文章原标题为 Molecular optimization by capturing chemist’s intuition using deep neural networks，文章使用通过 MMP 算法生成的分子对数据和分子性质数据训练了 Transformer 和 seq2seq 等模型，这些模型能够通过结构改造得到具有目标性质的分子。

## 方法

### 训练数据

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8505?authkey=ALYpzW-ZQ_VBXTU)

分子使用 SMILES 编码，原始分子的数据来自 ChEMBL，使用 MMP 算法对原始分子做化学改造，得到改造后的分子，也是训练模型时的目标分子，使用分子性质预测模型计算原始分子与目标分子的 LogD、溶解度以及 ADMET 性质。根据化学改造前后分子性质的变化，在原始分子前加上相应的性质标记，处理得到的两个分子构成一对 source-target 分子，作为训练集。

### 模型

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8507?authkey=ALYpzW-ZQ_VBXTU)

文章中所使用的 Transformer 和 seq2seq 都是在机器翻译领域取得了优异成绩的模型，机器翻译的任务中，需要将一段文本转换为另一段文本，将其应用在分子优化中，就是将原始分子序列优化为目标分子序列的任务视作为机器翻译，让模型找到原始分子与目标分子间的关系。

将 MMP 处理得到的分子对记作 $\{X,Y,Z\}$，其中 $X$ 表示原始分子，$Y$ 表示目标分子，$Z$ 表示从 $X$ 到 $Y$ 发生的性质改变，模型的训练过程就是要建立 $(X,Z)\rightarrow Y$ 的映射。

Transformer 与 seq2seq 两种模型的原理不同，seq2seq 模型具有编码器-解码器的构造，编码器将输入序列编码为变量，解码器通过该变量与前一时刻中的输出一起给出当前输出，直至序列结束，完成翻译过程，编码器-解码器的内部使用 RNN 实现。由于  RNN 结构的长期依赖问题，seq2seq 在处理长文本序列时能力有限。Transformer 也具有编码器-解码器的构造，但 Transformer 没有使用 RNN 并引入了 self-attention 机制，具有比 seq2seq 更强的文本处理能力。

### 评估指标

**目标 1：评估模型是否对指定的三个分子性质进行了优化**

考虑到性质预测模型的误差，三个性质都在以下范围内的分子视为完成了性质优化：

- $|\log D_\mathrm{generated}-\log D_\mathrm{target}|\leq 0.4$
- $\mathrm{solubility_{low}}\leq 2.3\ \mathrm{or}\ \mathrm{solubility_{high}}\geq 1.1$
- $\mathrm{clearance}=1.3\pm0.35$


**目标 2：评估模型是否基于 MMP 对分子做了化学改造**

使用 MMP 分析生成分子，检查分子结构是否符合要求。

为了评估模型生成分子的效果，文章制作了以下三个测试集：

1. Test-Original：从数据集中取出 10% 作为该测试集，但其中所有组合 $(X,Z)$ 都未在训练集中出现过。
2. Test-Molecule：Test-Origina 的子集，其中所有数据的分子都未在训练集中出现过，即 $\{(X,Z)_\mathrm{test}|X\notin \mathcal{X}_\mathrm{train}\}$。
3. Test-Property：由 Test-Original 中具有特定性质要求的组合构成，性质要求为 `LogD_change_=(-1.1,-0.9]`、`Solubility_low→high` 和 `CLint_high→low`。

## 结果

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8508?authkey=ALYpzW-ZQ_VBXTU)

使用三种测试集在不同模型上生成分子的结果如上图所示，首先可以看出，仅使用 MMP 算法对分子进行改造，也能得到一定数量具有目标特征的分子，说明 MMP 算法能够较好处理分子优化的问题，但 MMP 仅是对测试分子做没有方向性的、随机的改造，所以在生成分子的结果中，MMP 算法得到满足要求的分子也最少。

相比 MMP 算法，深度学习模型 HierG2G、seq2seq 和 Transformer 具有更好的效果，生成了更多满足要求的分子。其中 seq2seq 和 Transformer 具有比较接近的结果，且表现都优于 HierG2G，这可能是由于 HierG2G 是一种基于图的模型，对于「序列到序列」这一类型的任务表现不如专门用于文本处理的 seq2seq 和 Transformer。

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8509?authkey=ALYpzW-ZQ_VBXTU)

统计各个模型生成的分子，在满足目标性质与基于 MMP 对分子进行改造两方面的目标上，Transformer 生成分子中满足要求的分子都在高于另外两种模型，更好地完成了分子优化任务。

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8510?authkey=ALYpzW-ZQ_VBXTU)

考察训练集与使用不同测试集通过 Transformer 生成分子集中分子对之间的化学改造方式，上图展示了出现频率最高的 20 种改造方式。可以看出，在训练集、从 Test-Original 和 Test-Molecule 生成的分子集中，出现频率较高的几种分子改造方式都比较接近，这可以说明 Transformer 从训练集中学习到了 MMP 改造方式并将其应用于测试集从而生成分子。

Test-Property 中的分子改造方式与其他三种数据集中的方式差别较大，这是因为 Test-Property 数据集指定了特定的性质优化要求，在给定的性质单一优化方向下，Transformer 在学习到的 MMP 改造方式中选择最合适的方式，使生成分子的性质符合目标要求。这种工作方式与化学家改造分子的方式类似。

文章中使用模型所生成的分子如下图所示：

![!n](https://storage.live.com/items/4D18B16B8E0B1EDB!8511?authkey=ALYpzW-ZQ_VBXTU)

## 结论

文章使用 MMP 算法构建分子对数据集，MMP 算法模拟了现实中对单键切断并替换官能团的分子改造方式，因此可以训练得到与现实相似的分子改造模型。文章还考虑了分子改造的性质变化，现实中的分子改造大多是为了获得特定的性质，因此在结合了分子改造与性质变化后，文章构造了一种能够针对目标性质进行结构优化的模型，类似于现实中化学家对分子的优化。在试验的多种模型中，Transformer 获得了最佳的效果。但模型仍有许多改善的空间，例如只能对单点进行结构改造，能够指定的分子性质约束太少等等。