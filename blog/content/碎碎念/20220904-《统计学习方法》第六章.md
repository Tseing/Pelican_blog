title: 《统计学习方法》第五章：逻辑斯谛回归与最大熵模型
slug:  statistical-learning-chapter6
date: 2022-09-04
tags: 统计学习方法, Machine learning, Algorithm
summary: 《统计学习方法》第五章主要介绍逻辑斯谛回归模型与最大熵模型，这两种模型具有类似的对数结构，此外还都利用了极大似然估计原
status: draft

## 模型

### 逻辑斯谛回归模型

逻辑斯谛分布具有良好的性质，能够将 $(-\infty,+\infty)$ 映射至 $(-1，+1)$，因此选用逻辑斯谛分布作为回归模型。逻辑斯谛分布函数与密度函数为

$$\begin{align}
    F(x)&=P(\leqslant x)=\frac{1}{1+\mathrm{e}^{-(x-\mu)/\gamma}}\\
    f(x)&=F'(x)=\frac{\mathrm{e}^{-(x-\mu)/\gamma}}{\gamma(1+\mathrm{e}^{-(x-\mu)/\gamma})^2}
\end{align}$$

逻辑斯谛分布函数与密度函数的图像分别如下：

![逻辑斯谛分布](https://storage.live.com/items/4D18B16B8E0B1EDB!7554?authkey=ALYpzW-ZQ_VBXTU)

将逻辑斯谛分布函数简化，可以得到 Sigmoid 函数：

$$S(x)=\frac{1}{1+\mathrm{e}^{-x}}=\frac{\mathrm{e}^x}{1+\mathrm{e}^x}$$

回忆二分类的感知机 $w\cdot x+b$，超平面将实例分作 $w\cdot x+b\geqslant 0$ 与 $w\cdot x+b< 0$ 两类。可以看出，$w\cdot x+b$ 的值域为实数域，那么就可以利用逻辑斯谛分布将实数域映射到 $(-1，+1)$，实现分类。

为了表述简洁，令 $w=(w^{(1)},w^{(2)},\cdots,w^{(n)},b)^{\mathrm{T}}$，$x=(x^{(1)},x^{(2)},\cdots,x^{(n)},1)^{\mathrm{T}}$，将 $w\cdot x$ 代入 Sigmoid 函数：

$$\begin{align}
    P(Y=1|x)&=\frac{\exp(w\cdot x)}{1+\exp(w\cdot x)}\\
    P(Y=0|x)&=1-P(Y=1|x)=\frac{1}{1+\exp(w\cdot x)}
\end{align}$$

这就是二项逻辑斯谛回归模型。从式中也能看到，若线性函数 $w\cdot x$越大，$P(Y=1|x)$ 概率越大；若线性函数 $w\cdot x$越小，$P(Y=0|x)$ 概率越大。最后就通过对比 $P(Y=1|x)$ 与 $P(Y=0|x)$ 的大小来确定实例的类别。

### 最大熵模型

最大熵原理认为，熵最大的模型是最好的模型。这是一个十分在「直觉」的原理，例如说，在等待公交车时，下一辆公交车只有两种情况——「乘」或「不乘」，基于这种判断，通常会认为下一辆公交车有 50% 的概率可乘，50% 的概率不可乘。

再例如，某事件有 $\{A,B,C,D,E\}$ 5 种情况，相应满足约束：

$$P(A)+P(B)+P(C)+P(D)+P(E)=1$$

在没有更多信息的情况下，根据最大熵原理，我们会认为

$$P(A)=P(B)=P(C)=P(D)=P(E)=\frac{1}{5}$$

如果额外获得了信息 $P(A)+P(B)=\frac{3}{10}$，那么根据最大熵原理就会认为

$$P(A)=P(B)=\frac{3}{20},\ P(C)=P(D)=P(E)=\frac{7}{30}$$

可以看出，在缺少信息的情况下，最大熵原理将那些不确定的部分都视作等可能。等概率表示了对于事实的无知，但是没有更多信息，这种判断又是合理的。



## 策略

### 逻辑斯谛回归模型

设数据集中的频率为

$$P(Y=1|x)=\pi(x),\ P(Y=0|x)=1-\pi(x)$$

构造对数似然函数：

$$\begin{align}
    L(w)&=\log\prod_{i}[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}\\
    &=\sum_i[y_i\log\pi(x_i)+(1-y_i)\log(1-\pi(x_i))]\\
    &=\sum_i[y_i(w\cdot x_i)-\log(1+\exp(w\cdot x_i)]
\end{align}$$

那么求 $L(w)$ 的极大值，就能得到估计值 $\hat{w}$，得到回归模型。也就是说，求解逻辑斯谛回归模型就是对于对数似然函数的最优化问题。

{note begin}似然函数定义为 $L(p)=\prod_i p^{x_i}(1-p)^{1-x_i}$，即抽样结果中各频率之积。由于每次抽样独立同分布的前提，可以认为似然函数为抽样结果（该事件）发生的概率。因为已经得到了该抽样结果，该事件发生的概率理应为 1，所以就要使似然函数最大化，这就是<em>最大似然估计</em>的原理。{note end}