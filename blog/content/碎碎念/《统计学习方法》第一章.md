title: 《统计学习方法》第一章
slug: statistical-learning-chapter1
date: 2022-07-21
tags: 统计学习方法, Machine Learning
summary: 统计学习及监督学习概论

## 基本概念

### 统计学习过程

统计学习主要基于数据概率构建概率统计模型并运用模型分析和预测数据，统计学习的主要过程可以归纳为：

$$\boxed{\frac{训练数据集}{T=\{(x_i,y_i)\}}}\stackrel{学习}{\Longrightarrow}\boxed{\frac{模型}{P(Y|X)\ \mathrm{or}\ Y=f(X)}}\stackrel{预测}{\Longrightarrow}y_{N+1}$$

其中，训练集表示为：

$$
T=\{ (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}
$$

其中输入变量 $X$ 的取值 $x_i$ 为特征向量：

$$
x_i=(x^{(1)}_i,x^{(2)}_i,\cdots,x^{(n)}_i)^\mathrm{T}
$$

输入变量 $X$ 与输出变量 $Y$ 遵循联合概率分布 $P(X,Y)$，经过学习得到的模型可以是概率模型 $P(Y|X)$，也可以是非概率模型 $Y=f(X)$。

### 概率模型与非概率模型

在监督学习中，概率模型为条件概率分布形式 $P(y|x)$，非概率模型为决策函数形式 $y=f(x)$；在无监督学习中，概率模型为 $P(z|x)$ 或 $P(x|z)$，非概率模型为隐函数形式 $z=g(x)$。

$$P(y|x)\underset{归一化}{\overset{最大化}{\rightleftharpoons}} y=f(x)$$

由于 $P(y|x)$ 与 $y=f(x)$ 可以按照上述过程转化，所以概率模型与非概率模型不仅仅是在表现形式存在差异，更为重要的是在内部结构上，概率模型的变量、参数符合一定的联合概率分布，这也决定了概率模型符合以下基本概率公式：

$$P(x)=\sum_y P(x,y)$$

$$P(x,y)=P(x)P(y|x)$$

## 统计学习方法三要素

### 模型

模型就是需要学习的条件分布概率或决策函数，由假设模型构成的集合称为假设空间，可以表示为：

$$\mathcal{F}=\{f|Y=f_\theta(X),\theta\in\mathbf{R}^n\}$$

或

$$\mathcal{F}=\{P|P_\theta(Y|X),\theta\in\mathbf{R}^n\}$$

其中 $\theta$ 为模型的参数向量，$\mathbf{R}^n$ 称为参数空间。

### 策略

在假设空间中选择最优模型的方法称之为策略。

#### 损失函数和风险函数

用损失函数能够衡量模型预测值 $f(X)$ 与真实值 $Y$ 的差距，从而评估模型的优劣。损失函数记作 $L(Y,f(X))$，例如常见的平方损失函数定义为：

$$L(Y,f(X))=(Y-f(X))^2$$

输入变量 $X$ 与输出变量 $Y$ 遵循联合概率分布 $P(X,Y)$，损失函数的期望为：

$$\begin{align}
    R_{\mathrm{exp}}(f)&=E_P[L(Y,f(X))]\\
    &= \int_{\mathcal{X}\times\mathcal{Y}}L(y,f(x))P(x,y)\mathrm{d}x\mathrm{d}y\\
\end{align}$$

该期望就是模型 $f(X)$ 在联合分布 $P(X,Y)$ 平均意义下的损失，称为风险函数。但由于 $P(X,Y)$ 无法确定，风险函数也无法计算，因此定义了能够确定的经验风险。经验风险是模型 $f(X)$ 在训练集 $T$ 中的平均损失：

$$R_{\mathrm{rmp}}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))$$

当样本容量 $N$ 趋向于无穷时，经验风险 $R_{\mathrm{rmp}}$ 趋于期望风险 $R_{\mathrm{exp}}$

#### 经验风险最小化与结构风险最小化

经验风险最小化策略通过寻找经验风险最小的模型作为最优模型，经验风险最小化策略需要足够大的样本容量，否则容易出现过拟合。经验风险最小化策本质为求解最优化问题：

$$\min_{f\in\mathcal{F}}\frac{1}{N}L(y_i,f(x_i))$$

结构风险最小化策略是防止过拟合的策略，其本质为求解最优化问题：

$$\min_{f\in\mathcal{F}}\frac{1}{N}L(y_i,f(x_i))+\lambda J(f)$$

其中 $J(f)$ 是模型的复杂度，表示对复杂模型的惩罚，$\lambda$ 为系数。

### 算法

算法是学习模型的具体计算方法。

## 泛化

### 泛化误差

泛化能力是学习得到的模型对未知数据的预测能力，学习得到的模型对未知项进行预测的误差称为泛化误差，泛化误差等同于模型 $\hat{f}$ 的期望风险：

$$\begin{align}
    R_{\mathrm{exp}}(\hat{f})&=E_P[L(Y,\hat{f}(X))]\\
    &= \int_{\mathcal{X}\times\mathcal{Y}}L(y,\hat{f}(x))P(x,y)\mathrm{d}x\mathrm{d}y\\
\end{align}$$

### 二分类问题的泛化误差上界

**定理 1.1** 对于二分类问题，当假设空间是有限个函数的集合
 $\mathcal{F}=\{ f_1,f_2,\cdots,f_d\}$ 时，对任意一个函数 $f\in \mathcal{F}$，至少以概率 $1-\delta$，$0<\delta<1$，以下不等式成立（证明见参考）：

$$R(f)\leq\hat{R}(f)+\varepsilon(d,N,\delta)$$

$$\varepsilon(d,N\delta)=\sqrt{\frac{1}{2N}(\log d+\log\frac{1}{\delta})}$$

$R(f)$ 为泛化误差，$\hat{R}(f)$ 为训练误差。

泛化误差上界的性质：

- 样本容量增加，泛化上界趋于0；
- 假设空间越大，泛化上界越大。

## 监督学习的应用


| 输入变量 X | 输出变量 Y | 问题 |
|-----------|----------|------|
|           | 离散变量   | 分类 |
| 变量序列   | 变量序列   | 标注 |
| 连续变量   | 连续变量   | 回归 |

---

##References

- [李航, 2019. 统计学习方法（第2版）. 清华大学出版社.](https://book.douban.com/subject/33437381/)
- [二分类问题泛化误差上界的详细证明 - p_is_p - 博客园](https://www.cnblogs.com/pastispast/p/12589078.html)